{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "span_srl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9TyyvHf5cxT"
      },
      "source": [
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "\n",
        "sys.setrecursionlimit(100000000)\n",
        "theano.config.floatX = 'float32'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy3K6kub59Y6"
      },
      "source": [
        "Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBYfQ-FI5nsM"
      },
      "source": [
        "class Loader(object):\n",
        "    def __init__(self, argv):\n",
        "        self.argv = argv\n",
        "\n",
        "    def load(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def load_data(fn):\n",
        "        with gzip.open(fn, 'rb') as gf:\n",
        "            return pickle.load(gf)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_key_value_format(fn):\n",
        "        data = []\n",
        "        with open(fn, 'r') as f:\n",
        "            for line in f:\n",
        "                key, value = line.rstrip().split()\n",
        "                data.append((key, int(value)))\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def load_hdf5(path):\n",
        "        return h5py.File(path, 'r')\n",
        "\n",
        "    def load_txt_from_dir(self, dir_path, file_prefix):\n",
        "        file_names = get_file_names_in_dir(dir_path + '/*')\n",
        "        file_names = [fn for fn in file_names\n",
        "                      if os.path.basename(fn).startswith(file_prefix)\n",
        "                      and fn.endswith('txt')]\n",
        "        return [self.load(path=fn) for fn in file_names]\n",
        "\n",
        "    def load_hdf5_from_dir(self, dir_path, file_prefix):\n",
        "        file_names = get_file_names_in_dir(dir_path + '/*')\n",
        "        file_names = [fn for fn in file_names\n",
        "                      if os.path.basename(fn).startswith(file_prefix)\n",
        "                      and fn.endswith('hdf5')]\n",
        "        return [self.load_hdf5(fn) for fn in file_names]\n",
        "\n",
        "class Conll12Loader(Loader):\n",
        "\n",
        "    def load(self, path, data_size=1000000, is_test=False):\n",
        "        if path is None:\n",
        "            return []\n",
        "\n",
        "        corpus = []\n",
        "        sent = []\n",
        "\n",
        "        with open(path) as f:\n",
        "            for line in f:\n",
        "                elem = [l for l in line.rstrip().split()]\n",
        "                if len(elem) > 10:\n",
        "                    if is_test:\n",
        "                        sent.append(elem[:11])\n",
        "                    else:\n",
        "                        sent.append(elem)\n",
        "                elif len(elem) == 0:\n",
        "                    corpus.append(sent)\n",
        "                    sent = []\n",
        "                if len(corpus) >= data_size:\n",
        "                    break\n",
        "        return corpus"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r748E5pu6lpO"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFuDeuDRNzi"
      },
      "source": [
        "HYPH = u'-'\n",
        "UNK = u'UNKNOWN'\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self):\n",
        "        self.i2w = []\n",
        "        self.w2i = {}\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.w2i:\n",
        "            new_id = self.size()\n",
        "            self.i2w.append(word)\n",
        "            self.w2i[word] = new_id\n",
        "\n",
        "    def get_id(self, word):\n",
        "        return self.w2i.get(word)\n",
        "\n",
        "    def get_id_or_unk_id(self, word):\n",
        "        if word in self.w2i:\n",
        "            return self.w2i.get(word)\n",
        "        return self.w2i.get(UNK)\n",
        "\n",
        "    def get_and_add_id(self, word):\n",
        "        self.add_word(word)\n",
        "        return self.w2i.get(word)\n",
        "\n",
        "    def get_word(self, w_id):\n",
        "        return self.i2w[w_id]\n",
        "\n",
        "    def has_key(self, word):\n",
        "        return word in self.w2i\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.i2w)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoCQp_IWcro4"
      },
      "source": [
        "def save_key_value_format(fn, keys, values):\n",
        "    assert len(keys) == len(values)\n",
        "    if type(values[0]) is not str:\n",
        "        values = map(lambda v: str(v), values)\n",
        "    with open(fn + '.txt', 'w') as f:\n",
        "        for key, value in zip(keys, values):\n",
        "            f.write(\"%s\\t%s\\n\" % (key, value))\n",
        "\n",
        "def load_key_value_format(fn):\n",
        "    data = []\n",
        "    with open(fn, 'r') as f:\n",
        "        for line in f:\n",
        "            key, value = line.rstrip().split()\n",
        "            data.append((key, int(value)))\n",
        "    return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTQHJgRpapHN"
      },
      "source": [
        "class Sent(object):\n",
        "    def __init__(self, sent, is_test=True):\n",
        "        self.words = self._make_words(sent=sent, is_test=is_test)\n",
        "\n",
        "        self.forms = [word.form for word in self.words]\n",
        "        self.strings = [word.string for word in self.words]\n",
        "        self.marks = self._set_marks(self.words)\n",
        "        self.props = [word.prop for word in self.words]\n",
        "\n",
        "        self.prd_indices = self._set_prd_indices(self.marks)\n",
        "        self.prd_forms = [self.forms[i] for i in self.prd_indices]\n",
        "        self.prd_bio_labels = self._set_prd_bio_labels(self.props)\n",
        "        self.has_prds = True if len(self.prd_indices) > 0 else False\n",
        "\n",
        "        self.n_words = len(sent)\n",
        "        self.n_prds = len(self.prd_indices)\n",
        "\n",
        "        self.word_ids = None\n",
        "        self.mark_ids = None\n",
        "        self.elmo_emb = None\n",
        "        self.bio_label_ids = None\n",
        "        self.span_triples = None\n",
        "        self.span_triples_with_null = None\n",
        "\n",
        "    def _make_words(self, sent, is_test=True):\n",
        "        return [self._make_word(line, is_test) for line in sent]\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_word(line, is_test=True):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _set_marks(self, words):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_bio_labels(prop):\n",
        "        \"\"\"\n",
        "        :param prop: 1D: n_words; elem=bracket label\n",
        "        :return: 1D: n_words; elem=BIO label\n",
        "        \"\"\"\n",
        "        labels = []\n",
        "        prev = None\n",
        "        for arg in prop:\n",
        "            if arg.startswith('('):\n",
        "                if arg.endswith(')'):\n",
        "                    prev = arg.split(\"*\")[0][1:]\n",
        "                    label = 'B-' + prev\n",
        "                    prev = None\n",
        "                else:\n",
        "                    prev = arg[1:-1]\n",
        "                    label = 'B-' + prev\n",
        "            else:\n",
        "                if prev:\n",
        "                    label = 'I-' + prev\n",
        "                    if arg.endswith(')'):\n",
        "                        prev = None\n",
        "                else:\n",
        "                    label = 'O'\n",
        "            labels.append(label)\n",
        "        return labels\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_prd_indices(marks):\n",
        "        return [i for i, mark in enumerate(marks) if mark != HYPH]\n",
        "\n",
        "    def _set_prd_bio_labels(self, props):\n",
        "        \"\"\"\n",
        "        :param props: 1D: n_words, 2D: n_prds\n",
        "        :return: 1D: n_prds, 2D: n_words\n",
        "        \"\"\"\n",
        "        props = map(lambda p: p, zip(*props))\n",
        "        return [self._make_bio_labels(prop) for prop in props]\n",
        "\n",
        "    def set_word_ids(self, vocab_word):\n",
        "        self.word_ids = array(str_to_id(sent=self.forms,\n",
        "                                        vocab=vocab_word,\n",
        "                                        unk=UNK))\n",
        "\n",
        "    def set_mark_ids(self):\n",
        "        mark_ids = [[0 for _ in range(self.n_words)] for _ in range(self.n_prds)]\n",
        "        for i, prd_index in enumerate(self.prd_indices):\n",
        "            mark_ids[i][prd_index] = 1\n",
        "        self.mark_ids = array(mark_ids)\n",
        "\n",
        "    def set_label_ids(self, vocab_label):\n",
        "        \"\"\"\n",
        "        :param vocab_label: Vocab (BIO labels); e.g. B-A0, I-A0\n",
        "        \"\"\"\n",
        "        assert len(self.prd_indices) == len(self.prd_bio_labels)\n",
        "        label_ids = []\n",
        "        for prd_index, props in zip(self.prd_indices, self.prd_bio_labels):\n",
        "            y = str_to_id(sent=props, vocab=vocab_label, unk='O')\n",
        "            label_ids.append(y)\n",
        "        self.bio_label_ids = array(label_ids)\n",
        "\n",
        "    def set_elmo_emb(self, elmo_emb):\n",
        "        \"\"\"\n",
        "        :param elmo_emb: 1D: n_layers, 2D: n_words, 3D: dim\n",
        "        \"\"\"\n",
        "        elmo_emb = np.asarray(elmo_emb)\n",
        "        elmo_emb = elmo_emb.transpose((1, 0, 2))\n",
        "        assert len(elmo_emb) == self.n_words\n",
        "        self.elmo_emb = elmo_emb\n",
        "\n",
        "    def set_span_triples(self, vocab_label):\n",
        "        \"\"\"\n",
        "        :param vocab_label: Vocab (labels); e.g. A0, A1\n",
        "        \"\"\"\n",
        "        triples = []\n",
        "        for bio_labels in self.prd_bio_labels:\n",
        "            prd_triples = []\n",
        "            for (label, i, j) in self._get_spans(bio_labels):\n",
        "                r = vocab_label.get_id(label)\n",
        "                prd_triples.append((r, i, j))\n",
        "            triples.append(prd_triples)\n",
        "        self.span_triples = triples\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_spans(bio_labels):\n",
        "        \"\"\"\n",
        "        :param bio_labels: 1D: n_words; elem=bio label\n",
        "        :return: 1D: n_spans; elem=[label, i, j]\n",
        "        \"\"\"\n",
        "        spans = []\n",
        "        span = []\n",
        "        for i, label in enumerate(bio_labels):\n",
        "            if label[-2:] == '-V':\n",
        "                continue\n",
        "            if label.startswith('B-'):\n",
        "                if span:\n",
        "                    spans.append(span)\n",
        "                span = [label[2:], i, i]\n",
        "            elif label.startswith('I-'):\n",
        "                if span:\n",
        "                    if label[2:] == span[0]:\n",
        "                        span[2] = i\n",
        "                    else:\n",
        "                        spans.append(span)\n",
        "                        span = [label[2:], i, i]\n",
        "                else:\n",
        "                    span = [label[2:], i, i]\n",
        "            else:\n",
        "                if span:\n",
        "                    spans.append(span)\n",
        "                span = []\n",
        "        if span:\n",
        "            spans.append(span)\n",
        "        return spans\n",
        "\n",
        "    def set_span_triples_with_null(self, n_labels):\n",
        "        assert len(self.span_triples) == len(self.prd_indices)\n",
        "        triples_with_null = []\n",
        "        for prd_index, spans in zip(self.prd_indices, self.span_triples):\n",
        "            used_labels = [r for (r, i, j) in spans]\n",
        "            null_spans = [(r, prd_index, prd_index)\n",
        "                          for r in range(n_labels)\n",
        "                          if r not in used_labels]\n",
        "            triples = spans + null_spans\n",
        "            triples.sort(key=lambda s: s[0])\n",
        "            triples_with_null.append(triples)\n",
        "        self.span_triples_with_null = triples_with_null\n",
        "\n",
        "class Conll12Sent(Sent):\n",
        "    @staticmethod\n",
        "    def _make_word(line, is_test=False):\n",
        "        return Word(form=line[3],\n",
        "                    mark=line[6],\n",
        "                    sense=line[7],\n",
        "                    prop=line[11:-1] if is_test is False else [])\n",
        "\n",
        "    def _set_marks(self, words):\n",
        "        return list(map(lambda w: w.mark if w.sense != HYPH else HYPH, words))\n",
        "\n",
        "class Word(object):\n",
        "    def __init__(self, form, mark, sense, prop):\n",
        "        self.form = form.lower()\n",
        "        self.string = form\n",
        "        self.mark = mark\n",
        "        self.sense = sense\n",
        "        self.prop = prop"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYEhoSOw58AC"
      },
      "source": [
        "class Preprocessor(object):\n",
        "    def __init__(self, argv):\n",
        "        self.argv = argv\n",
        "        self.data_type = argv.data_type\n",
        "\n",
        "    @staticmethod\n",
        "    def make_vocab_word(word_list):\n",
        "        vocab_word = Vocab()\n",
        "        vocab_word.add_word(UNK)\n",
        "        for w in word_list:\n",
        "            vocab_word.add_word(w)\n",
        "        return vocab_word\n",
        "\n",
        "    def make_and_save_vocab_label(self,\n",
        "                                  sents,\n",
        "                                  vocab_label_init=None,\n",
        "                                  save=False,\n",
        "                                  load=False):\n",
        "        argv = self.argv\n",
        "\n",
        "        if load and argv.load_label:\n",
        "            label_key_value = load_key_value_format(argv.load_label)\n",
        "            vocab_label = make_vocab_from_ids(label_key_value)\n",
        "        else:\n",
        "            vocab_label = self.make_vocab_label(sents=sents,\n",
        "                                                vocab_label_init=vocab_label_init)\n",
        "        if save:\n",
        "            if argv.output_dir:\n",
        "                dir_name = argv.output_dir\n",
        "            else:\n",
        "                dir_name = 'output'\n",
        "            if argv.output_fn:\n",
        "                file_name = '/label_ids.' + argv.output_fn\n",
        "            else:\n",
        "                file_name = '/label_ids'\n",
        "\n",
        "            fn = dir_name + file_name\n",
        "            values, keys = map(lambda x: x, zip(*enumerate(vocab_label.i2w)))\n",
        "            save_key_value_format(fn=fn, keys=keys, values=values)\n",
        "\n",
        "        return vocab_label\n",
        "\n",
        "    def make_sents(self, corpus):\n",
        "        \"\"\"\n",
        "        :param corpus: 1D: n_sents, 2D: n_words\n",
        "        :return: 1D: n_sents\n",
        "        \"\"\"\n",
        "        if len(corpus) == 0:\n",
        "            return []\n",
        "\n",
        "        if self.data_type == 'conll05':\n",
        "            column = 6\n",
        "            gen_sent = Conll05Sent\n",
        "        else:\n",
        "            column = 12\n",
        "            gen_sent = Conll12Sent\n",
        "\n",
        "        is_test = True if len(corpus[0][0]) < column else False\n",
        "        return [gen_sent(sent, is_test) for sent in corpus]\n",
        "\n",
        "    @staticmethod\n",
        "    def split_x_and_y(batches, index=-1):\n",
        "        \"\"\"\n",
        "        :param batches: 1D: n_batches, 2D: batch_size; elem=(x, m, y)\n",
        "        :param index: split column index\n",
        "        :return 1D: n_batches, 2D: batch_size; elem=(x, m)\n",
        "        :return 1D: n_batches, 2D: batch_size; elem=y\n",
        "        \"\"\"\n",
        "        x = []\n",
        "        y = []\n",
        "        for batch in batches:\n",
        "            x.append(batch[:index])\n",
        "            y.append(batch[index])\n",
        "        return x, y\n",
        "\n",
        "    def make_batches(self,\n",
        "                     samples,\n",
        "                     is_valid_data=False,\n",
        "                     shuffle=True):\n",
        "        \"\"\"\n",
        "        :param samples: 1D: n_samples, 2D: [x, m, y]\n",
        "        :param is_valid_data: boolean\n",
        "        :param shuffle: boolean\n",
        "        :return 1D: n_batches, 2D: batch_size; elem=[x, m, y]\n",
        "        \"\"\"\n",
        "        if shuffle:\n",
        "            np.random.shuffle(samples)\n",
        "            samples.sort(key=lambda sample: len(sample[0]))\n",
        "\n",
        "        batches = []\n",
        "        batch = []\n",
        "        prev_n_words = len(samples[0][0])\n",
        "\n",
        "        for sample in samples:\n",
        "            n_words = len(sample[0])\n",
        "            if len(batch) == self.argv.batch_size or prev_n_words != n_words:\n",
        "                batches.append(self._make_one_batch(batch, is_valid_data))\n",
        "                batch = []\n",
        "                prev_n_words = n_words\n",
        "            batch.append(sample)\n",
        "\n",
        "        if batch:\n",
        "            batches.append(self._make_one_batch(batch, is_valid_data))\n",
        "\n",
        "        if shuffle:\n",
        "            np.random.shuffle(batches)\n",
        "\n",
        "        for batch in batches:\n",
        "            yield batch\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_one_batch(batch, is_valid_data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def make_batch_per_sent(sents):\n",
        "        \"\"\"\n",
        "        :param sents: 1D: n_sents; Sent()\n",
        "        :return 1D: n_sents, 2D: n_prds; elem=[x, m]\n",
        "        \"\"\"\n",
        "        batches = []\n",
        "        for sent in sents:\n",
        "            x = []\n",
        "\n",
        "            x_word_ids = sent.word_ids\n",
        "            if x_word_ids is not None:\n",
        "                x.append(x_word_ids)\n",
        "\n",
        "            x_elmo_emb = sent.elmo_emb\n",
        "            if x_elmo_emb is not None:\n",
        "                x.append(x_elmo_emb)\n",
        "\n",
        "            batch = list(map(lambda m: x + [m], sent.mark_ids))\n",
        "            batches.append(list(map(lambda b: b, zip(*batch))))\n",
        "\n",
        "        return batches\n",
        "\n",
        "    @staticmethod\n",
        "    def set_sent_config(sents, elmo_emb, vocab_word, vocab_label):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def make_samples(sents, is_valid_data=False):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def make_vocab_label(self,\n",
        "                         sents,\n",
        "                         vocab_label_init=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SpanPreprocessor(Preprocessor):\n",
        "    def make_vocab_label(self,\n",
        "                         sents,\n",
        "                         vocab_label_init=None):\n",
        "        if len(sents) == 0:\n",
        "            return None\n",
        "\n",
        "        if vocab_label_init:\n",
        "            vocab_label = deepcopy(vocab_label_init)\n",
        "        else:\n",
        "            vocab_label = Vocab()\n",
        "            if self.argv.data_type == 'conll05':\n",
        "                core_labels = [\"A0\", \"A1\", \"A2\", \"A3\", \"A4\", \"A5\"]\n",
        "            else:\n",
        "                core_labels = [\"ARG0\", \"ARG1\", \"ARG2\", \"ARG3\", \"ARG4\", \"ARG5\"]\n",
        "            for label in core_labels:\n",
        "                vocab_label.add_word(label)\n",
        "\n",
        "        bio_labels = []\n",
        "        for sent in sents:\n",
        "            for props in sent.prd_bio_labels:\n",
        "                bio_labels += props\n",
        "        cnt = Counter(bio_labels)\n",
        "        bio_labels = [(w, c) for w, c in cnt.most_common()]\n",
        "\n",
        "        for label, count in bio_labels:\n",
        "            if not label.endswith('-V') and len(label) > 1:\n",
        "                vocab_label.add_word(label[2:])\n",
        "\n",
        "        return vocab_label\n",
        "\n",
        "    @staticmethod\n",
        "    def set_sent_config(sents, elmo_emb, vocab_word, vocab_label):\n",
        "        for index, sent in enumerate(sents):\n",
        "            sent.set_mark_ids()\n",
        "            if vocab_word:\n",
        "                sent.set_word_ids(vocab_word)\n",
        "            if elmo_emb:\n",
        "                sent.set_elmo_emb(elmo_emb[str(index)])\n",
        "            if vocab_label:\n",
        "                sent.set_span_triples(vocab_label)\n",
        "                sent.set_span_triples_with_null(vocab_label.size())\n",
        "        return sents\n",
        "\n",
        "    @staticmethod\n",
        "    def make_samples(sents, is_valid_data=False):\n",
        "        samples = []\n",
        "\n",
        "        for sent in sents:\n",
        "            x = []\n",
        "\n",
        "            x_word_ids = sent.word_ids\n",
        "            if x_word_ids is not None:\n",
        "                x.append(x_word_ids)\n",
        "\n",
        "            x_elmo_emb = sent.elmo_emb\n",
        "            if x_elmo_emb is not None:\n",
        "                x.append(x_elmo_emb)\n",
        "\n",
        "            if is_valid_data:\n",
        "                triples = sent.span_triples\n",
        "            else:\n",
        "                triples = sent.span_triples_with_null\n",
        "\n",
        "            assert len(sent.mark_ids) == len(triples)\n",
        "            for m, spans in zip(sent.mark_ids, triples):\n",
        "                # spans: 1D: n_spans, 2D: (r, i, j)\n",
        "                samples.append(x + [m, spans])\n",
        "\n",
        "        return samples\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_one_batch(batch, is_valid_data):\n",
        "        if is_valid_data:\n",
        "            return list(map(lambda b: b, zip(*batch)))\n",
        "\n",
        "        b = []\n",
        "        y = []\n",
        "        n_words = len(batch[0][0])\n",
        "        for b_index, sample in enumerate(batch):\n",
        "            b.append(sample[:-1])\n",
        "            y_tmp = []\n",
        "            for (r, i, j) in sample[-1]:\n",
        "                span_index = span_to_span_index(i, j, n_words)\n",
        "                y_tmp.append([b_index, r, span_index])\n",
        "            y += y_tmp\n",
        "\n",
        "        x = list(map(lambda b_i: b_i, zip(*b)))\n",
        "\n",
        "        return x + [y]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXUUaeNK87bA"
      },
      "source": [
        "Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3SziD3l6rD7"
      },
      "source": [
        "def f_score(crr_total, p_total, r_total):\n",
        "    precision = crr_total / p_total if p_total > 0 else 0.\n",
        "    recall = crr_total / r_total if r_total > 0 else 0.\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0.\n",
        "    return precision, recall, f1\n",
        "\n",
        "def concat_c_spans_from_spans(spans, vocab_label):\n",
        "    spans = [[vocab_label.get_word(l), i, j] for (l, i, j) in spans]\n",
        "    labels = [l for (l, i, j) in spans]\n",
        "    c_indices = [index for index, (l, i, j) in enumerate(spans) if l.startswith('C')]\n",
        "    non_ant_c_spans = []\n",
        "\n",
        "    for c_index in c_indices:\n",
        "        c_span = spans[c_index]\n",
        "        label = c_span[0][2:]\n",
        "        if label in labels:\n",
        "            spans[labels.index(label)].extend(c_span[1:])\n",
        "\n",
        "    concated_spans = [span for i, span in enumerate(spans) if i not in c_indices]\n",
        "    spans = concated_spans + non_ant_c_spans\n",
        "    return spans\n",
        "\n",
        "class Evaluator(object):\n",
        "    def __init__(self, argv):\n",
        "        self.argv = argv\n",
        "\n",
        "    def f_score(self, y_true, y_pred, vocab_label):\n",
        "        \"\"\"\n",
        "        :param y_true: 1D: n_batches, 2D: batch_size, 3D: n_spans, 4D: [label_id, pre_index, post_index]\n",
        "        :param y_pred: 1D: n_batches, 2D: batch_size, 3D: n_spans, 4D: [label_id, pre_index, post_index]\n",
        "        \"\"\"\n",
        "        correct, p_total, r_total = self.metrics(y_true=y_true,\n",
        "                                                 y_pred=y_pred,\n",
        "                                                 vocab_label=vocab_label)\n",
        "        p, r, f = f_score(correct, p_total, r_total)\n",
        "        write('\\tF:{:>7.2%}  P:{:>7.2%} ({:>5}/{:>5})  R:{:>7.2%} ({:>5}/{:>5})'.format(\n",
        "            f, p, int(correct), int(p_total), r, int(correct), int(r_total))\n",
        "        )\n",
        "        return f\n",
        "\n",
        "    def metrics(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SpanEvaluator(Evaluator):\n",
        "    def metrics(self, y_true, y_pred, vocab_label):\n",
        "        \"\"\"\n",
        "        :param y_true: 1D: n_batches, 2D: batch_size, 3D: n_spans, 4D: [label_id, pre_index, post_index]\n",
        "        :param y_pred: 1D: n_batches, 2D: batch_size, 3D: n_spans, 4D: [label_id, pre_index, post_index]\n",
        "        \"\"\"\n",
        "        p_total = 0.\n",
        "        r_total = 0.\n",
        "        correct = 0.\n",
        "        for span_true_batch, span_pred_batch in zip(y_true, y_pred):\n",
        "            for spans_true, spans_pred in zip(span_true_batch, span_pred_batch):\n",
        "                spans_true = concat_c_spans_from_spans(spans_true, vocab_label)\n",
        "                spans_pred = concat_c_spans_from_spans(spans_pred, vocab_label)\n",
        "                p_total += len(spans_pred)\n",
        "                r_total += len(spans_true)\n",
        "                for span in spans_pred:\n",
        "                    if span in spans_true:\n",
        "                        correct += 1\n",
        "        return correct, p_total, r_total\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXVFY12l822i"
      },
      "source": [
        "Decoder part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55SmJ9NG8_yz"
      },
      "source": [
        "from copy import deepcopy\n",
        "from itertools import combinations_with_replacement as comb\n",
        "\n",
        "def span_to_span_index(i, j, n_words):\n",
        "    return i * (n_words - 1) + j - np.arange(i).sum()\n",
        "\n",
        "class Decoder(object):\n",
        "    def __init__(self, argv, vocab_label):\n",
        "        self.argv = argv\n",
        "        self.core_label_ids = self.set_core_labels(vocab_label)\n",
        "        self.span_list = None\n",
        "\n",
        "    def set_core_labels(self, vocab_label):\n",
        "        if self.argv.data_type == 'conll05':\n",
        "            core_labels = [\"A0\", \"A1\", \"A2\", \"A3\", \"A4\", \"A5\"]\n",
        "        else:\n",
        "            core_labels = [\"ARG0\", \"ARG1\", \"ARG2\", \"ARG3\", \"ARG4\", \"ARG5\"]\n",
        "        return [vocab_label.get_id(label)\n",
        "                for label in core_labels\n",
        "                if vocab_label.has_key(label)]\n",
        "\n",
        "    def argmax_span_triples(self, span_indices, marks):\n",
        "        \"\"\"\n",
        "        :param span_indices: 1D: batch_size, 2D; n_labels; span index\n",
        "        :param marks: 1D: batch_size, 2D; n_words\n",
        "        :return: 1D: batch_size, 2D: n_spans; [r, i, j]\n",
        "        \"\"\"\n",
        "        n_words = len(marks[0])\n",
        "        self.span_list = list(comb(range(n_words), 2))\n",
        "        return [self._argmax_search(span_indices_i, mark)\n",
        "                for span_indices_i, mark in zip(span_indices, marks)]\n",
        "\n",
        "    def _argmax_search(self, span_indices, mark):\n",
        "        spans = []\n",
        "        prd_index = mark.nonzero()[0][0]\n",
        "        for r, span_index in enumerate(span_indices):\n",
        "            (i, j) = self.span_list[span_index]\n",
        "            if i <= prd_index <= j:\n",
        "                continue\n",
        "            spans.append([r, i, j])\n",
        "        return spans\n",
        "\n",
        "    def greedy_span_triples(self, scores, marks):\n",
        "        \"\"\"\n",
        "        :param scores: 1D: batch_size, 2D; n_labels, 3D: n_spans; score\n",
        "        :param marks: 1D: batch_size, 2D; n_words\n",
        "        :return: 1D: batch_size, 2D: n_spans; [r, i, j]\n",
        "        \"\"\"\n",
        "        n_words = len(marks[0])\n",
        "        self.span_list = list(comb(range(n_words), 2))\n",
        "        return [self._greedy_search(score, mark)\n",
        "                for score, mark in zip(scores, marks)]\n",
        "\n",
        "    def _greedy_search(self, scores, mark):\n",
        "        \"\"\"\n",
        "        :param scores: 1D: n_labels, 2D: n_spans; score\n",
        "        :param mark: 1D: n_words; elem=0/1\n",
        "        :return: 1D: n_spans, 2D: [r, i, j]\n",
        "        \"\"\"\n",
        "        triples = []\n",
        "        used_words = deepcopy(mark)\n",
        "        used_labels = []\n",
        "\n",
        "        n_words = len(mark)\n",
        "        prd_index = mark.nonzero()[0][0]\n",
        "        prd_span_index = span_to_span_index(i=prd_index,\n",
        "                                            j=prd_index,\n",
        "                                            n_words=n_words)\n",
        "        spans = self._sort_spans(scores=scores,\n",
        "                                 prd_index=prd_index,\n",
        "                                 prd_span_index=prd_span_index)\n",
        "\n",
        "        for (r, i, j, _) in spans:\n",
        "            if r in used_labels:\n",
        "                continue\n",
        "            if used_words[i: j + 1].sum() > 0:\n",
        "                continue\n",
        "\n",
        "            triples.append([r, i, j])\n",
        "\n",
        "            used_words[i: j + 1] = 1\n",
        "            if r in self.core_label_ids:\n",
        "                used_labels.append(r)\n",
        "\n",
        "        return triples\n",
        "\n",
        "    def _sort_spans(self, scores, prd_index, prd_span_index):\n",
        "        \"\"\"\n",
        "        :param scores: 1D: n_labels, 2D: n_spans; score\n",
        "        :return: 1D: n_labels, 2D: n_words * n_words; elem=(r, i, j, score)\n",
        "        \"\"\"\n",
        "        spans = []\n",
        "        for r, scores_row in enumerate(scores):\n",
        "            score_prd = scores_row[prd_span_index]\n",
        "            for index, score in enumerate(scores_row):\n",
        "                (i, j) = self.span_list[index]\n",
        "                if i <= prd_index <= j:\n",
        "                    continue\n",
        "                if score_prd < score:\n",
        "                    spans.append((r, i, j, score))\n",
        "        spans.sort(key=lambda span: span[-1], reverse=True)\n",
        "        return spans"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxcdKCTZLmUT"
      },
      "source": [
        "class Initializer(object):\n",
        "    def __call__(self, shape, shared=True, name=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class Zero(Initializer):\n",
        "    def __call__(self, shape, shared=True, name=None):\n",
        "        param = np.zeros(shape, theano.config.floatX)\n",
        "        if shared:\n",
        "            return theano.shared(value=param, name=name, borrow=True)\n",
        "        return param\n",
        "\n",
        "\n",
        "class One(Initializer):\n",
        "    def __call__(self, shape, shared=True, name=None):\n",
        "        param = np.ones(shape, theano.config.floatX)\n",
        "        if shared:\n",
        "            return theano.shared(value=param, name=name, borrow=True)\n",
        "        return param\n",
        "\n",
        "\n",
        "class Identity(Initializer):\n",
        "    def __call__(self, shape, shared=True, name=None):\n",
        "        assert len(shape) == 2\n",
        "        param = np.ones(shape[0], theano.config.floatX)\n",
        "        param = np.diag(param)\n",
        "        if shared:\n",
        "            return theano.shared(value=param, name=name, borrow=True)\n",
        "        return param\n",
        "\n",
        "class Uniform(Initializer):\n",
        "    def __call__(self, shape, shared=True, name=None):\n",
        "        param = np.asarray(np.random.uniform(low=-0.01,\n",
        "                                             high=0.01,\n",
        "                                             size=shape),\n",
        "                           dtype=theano.config.floatX)\n",
        "        if shared:\n",
        "            return theano.shared(value=param, name=name, borrow=True)\n",
        "        return param\n",
        "\n",
        "\n",
        "class Normal(Initializer):\n",
        "    def __call__(self, shape, shared=True, name=None):\n",
        "        param = np.asarray(np.random.normal(0.0, 0.01, shape),\n",
        "                           dtype=theano.config.floatX)\n",
        "        if shared:\n",
        "            return theano.shared(value=param, name=name, borrow=True)\n",
        "        return param\n",
        "\n",
        "\n",
        "class Xavier(Initializer):\n",
        "    def __call__(self, shape, shared=True, name=None):\n",
        "        param = np.asarray(np.random.uniform(low=-np.sqrt(6.0 / np.sum(shape)),\n",
        "                                             high=np.sqrt(6.0 / np.sum(shape)),\n",
        "                                             size=shape),\n",
        "                           dtype=theano.config.floatX)\n",
        "        if shared:\n",
        "            return theano.shared(value=param, name=name, borrow=True)\n",
        "        return param\n",
        "\n",
        "\n",
        "class Orthonormal(Initializer):\n",
        "    \n",
        "    def __call__(self, shape, shared=True, name=None):\n",
        "        assert len(shape) == 2\n",
        "        if shape[0] == shape[1]:\n",
        "            M = np.random.randn(*shape).astype(theano.config.floatX)\n",
        "            Q, R = np.linalg.qr(M)\n",
        "            Q = Q * np.sign(np.diag(R))\n",
        "            param = Q * 1.0\n",
        "        else:\n",
        "            M1 = np.random.randn(shape[0], shape[0]).astype(theano.config.floatX)\n",
        "            M2 = np.random.randn(shape[1], shape[1]).astype(theano.config.floatX)\n",
        "            Q1, R1 = np.linalg.qr(M1)\n",
        "            Q2, R2 = np.linalg.qr(M2)\n",
        "            Q1 = Q1 * np.sign(np.diag(R1))\n",
        "            Q2 = Q2 * np.sign(np.diag(R2))\n",
        "            n_min = min(shape[0], shape[1])\n",
        "            param = np.dot(Q1[:, :n_min], Q2[:n_min, :]) * 1.0\n",
        "        if shared:\n",
        "            return theano.shared(value=param, name=name, borrow=True)\n",
        "        return param\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 3:\n",
        "        x_shape = x.shape\n",
        "        x = x.reshape((x_shape[0] * x_shape[1], x_shape[2]))\n",
        "        return T.nnet.softmax(x).reshape(x_shape)\n",
        "    elif x.ndim == 4:\n",
        "        x_shape = x.shape\n",
        "        x = x.reshape((x_shape[0] * x_shape[1] * x_shape[2], x_shape[3]))\n",
        "        return T.nnet.softmax(x).reshape(x_shape)\n",
        "    return T.nnet.softmax(x)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return T.nnet.sigmoid(x)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return T.tanh(x)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return T.nnet.relu(x)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhwyRjcxLCzT"
      },
      "source": [
        "\n",
        "class Unit(object):\n",
        "    def __init__(self, name='unit'):\n",
        "        self.name = name\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_param(shape, init_type=None, name=None):\n",
        "        if init_type == 'zero':\n",
        "            init = Zero()\n",
        "        elif init_type == 'one':\n",
        "            init = One()\n",
        "        elif init_type == 'xavier':\n",
        "            init = Xavier()\n",
        "        elif init_type == 'orth':\n",
        "            init = Orthonormal()\n",
        "        elif init_type == 'identity':\n",
        "            init = Identity()\n",
        "        elif init_type == 'uniform':\n",
        "            init = Uniform()\n",
        "        else:\n",
        "            init = Normal()\n",
        "        return init(shape=shape, name=name)\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_activation(activation_type):\n",
        "        if activation_type == 'sigmoid':\n",
        "            return sigmoid\n",
        "        elif activation_type == 'tanh':\n",
        "            return tanh\n",
        "        elif activation_type == 'relu':\n",
        "            return relu\n",
        "        elif activation_type == 'softmax':\n",
        "            return softmax\n",
        "        return None\n",
        "\n",
        "class Dense(Unit):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 output_dim,\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 weight_init='xavier',\n",
        "                 bias_init='zero'):\n",
        "        super(Dense, self).__init__(name='Dense(%dx%d,%s)' % (input_dim, output_dim, activation))\n",
        "\n",
        "        self.W = self._set_param(shape=(input_dim, output_dim),\n",
        "                                 init_type=weight_init,\n",
        "                                 name='W_dense')\n",
        "        if use_bias:\n",
        "            self.b = self._set_param(shape=output_dim,\n",
        "                                     init_type=bias_init,\n",
        "                                     name='b_dense')\n",
        "            self.params = [self.W, self.b]\n",
        "        else:\n",
        "            self.b = None\n",
        "            self.params = [self.W]\n",
        "\n",
        "        self.activation = self._set_activation(activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = T.dot(x, self.W)\n",
        "        if self.b:\n",
        "            h = h + self.b\n",
        "        if self.activation:\n",
        "            h = self.activation(h)\n",
        "        return h\n",
        "\n",
        "class Dropout(Unit):\n",
        "    \"\"\"\n",
        "    Reference: [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n",
        "    \"\"\"\n",
        "    def __init__(self, rate, seed=0):\n",
        "        super(Dropout, self).__init__(name='Dropout(p={:>1.1})'.format(rate))\n",
        "        self.rate = min(1., max(0., rate))\n",
        "        self.srng = T.shared_randomstreams.RandomStreams(seed=seed)\n",
        "\n",
        "    def forward(self, x, is_train):\n",
        "        drop_mask = self.srng.binomial(size=x.shape, n=1, p=1 - self.rate, dtype=theano.config.floatX)\n",
        "        return T.switch(T.eq(is_train, 1), x * drop_mask, x * (1 - self.rate))\n",
        "\n",
        "class Embedding(Unit):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 output_dim,\n",
        "                 init_emb=None,\n",
        "                 param_init='xavier',\n",
        "                 param_fix=False,\n",
        "                 drop_rate=0.0,\n",
        "                 name=None):\n",
        "        super(Embedding, self).__init__(name=name if name else 'Emb(%dx%d)' % (input_dim, output_dim))\n",
        "        self.dropout = Dropout(drop_rate)\n",
        "\n",
        "        self.W = self._set_weight(input_dim, output_dim, init_emb, param_init)\n",
        "        if param_fix:\n",
        "            self.params = []\n",
        "        else:\n",
        "            self.params = [self.W]\n",
        "\n",
        "    def _set_weight(self, input_dim, output_dim, init_emb, param_init):\n",
        "        if init_emb is None:\n",
        "            return self._set_param(shape=(input_dim, output_dim),\n",
        "                                   init_type=param_init,\n",
        "                                   name='embedding')\n",
        "        return theano.shared(init_emb)\n",
        "\n",
        "    def forward(self, x, is_train=0):\n",
        "        return self.dropout.forward(x=self.W[x], is_train=is_train)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT2NGQ3ntZ3o"
      },
      "source": [
        "LSTM Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUfBT7EHMrW-"
      },
      "source": [
        "class LSTM(Unit):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 output_dim,\n",
        "                 use_bias=True,\n",
        "                 recurrent_init='orth',\n",
        "                 bias_init='zero'):\n",
        "        super(LSTM, self).__init__(name='LSTM(%dx%d)' % (input_dim, output_dim))\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # inout gate parameters\n",
        "        self.W_xi = self._set_param(shape=(input_dim, output_dim),\n",
        "                                    init_type=recurrent_init,\n",
        "                                    name='W_xi')\n",
        "        self.W_hi = self._set_param(shape=(output_dim, output_dim),\n",
        "                                    init_type=recurrent_init,\n",
        "                                    name='W_hi')\n",
        "        self.W_ci = self._set_param(shape=output_dim,\n",
        "                                    init_type='xavier',\n",
        "                                    name='W_ci')\n",
        "\n",
        "        # forget gate parameters\n",
        "        self.W_xf = self._set_param(shape=(input_dim, output_dim),\n",
        "                                    init_type=recurrent_init,\n",
        "                                    name='W_xf')\n",
        "        self.W_hf = self._set_param(shape=(output_dim, output_dim),\n",
        "                                    init_type=recurrent_init,\n",
        "                                    name='W_hf')\n",
        "        self.W_cf = self._set_param(shape=output_dim,\n",
        "                                    init_type='xavier',\n",
        "                                    name='W_cf')\n",
        "\n",
        "        # cell parameters\n",
        "        self.W_xc = self._set_param(shape=(input_dim, output_dim),\n",
        "                                    init_type=recurrent_init,\n",
        "                                    name='W_xc')\n",
        "        self.W_hc = self._set_param(shape=(output_dim, output_dim),\n",
        "                                    init_type=recurrent_init,\n",
        "                                    name='W_hc')\n",
        "\n",
        "        # output gate parameters\n",
        "        self.W_xo = self._set_param(shape=(input_dim, output_dim),\n",
        "                                    init_type=recurrent_init,\n",
        "                                    name='W_xf')\n",
        "        self.W_ho = self._set_param(shape=(output_dim, output_dim),\n",
        "                                    init_type=recurrent_init,\n",
        "                                    name='W_hf')\n",
        "        self.W_co = self._set_param(shape=output_dim,\n",
        "                                    init_type='xavier',\n",
        "                                    name='W_cf')\n",
        "\n",
        "        if use_bias:\n",
        "            self.b_xi = self._set_param(shape=output_dim,\n",
        "                                        init_type=bias_init,\n",
        "                                        name='b_xi')\n",
        "            self.b_xf = self._set_param(shape=output_dim,\n",
        "                                        init_type='one',\n",
        "                                        name='b_xf')\n",
        "            self.b_xc = self._set_param(shape=output_dim,\n",
        "                                        init_type=bias_init,\n",
        "                                        name='b_xc')\n",
        "            self.b_xo = self._set_param(shape=output_dim,\n",
        "                                        init_type=bias_init,\n",
        "                                        name='b_xo')\n",
        "            self.params = [self.W_xi, self.W_hi, self.W_ci, self.W_xf, self.W_hf, self.W_cf,\n",
        "                           self.W_xc, self.W_hc, self.W_xo, self.W_ho, self.W_co,\n",
        "                           self.b_xi, self.b_xf, self.b_xc, self.b_xo]\n",
        "        else:\n",
        "            self.b_xi = None\n",
        "            self.b_xf = None\n",
        "            self.b_xc = None\n",
        "            self.b_xo = None\n",
        "            self.params = [self.W_xi, self.W_hi, self.W_ci, self.W_xf, self.W_hf, self.W_cf,\n",
        "                           self.W_xc, self.W_hc, self.W_xo, self.W_ho, self.W_co]\n",
        "\n",
        "    def _step(self, xi_t, xf_t, xc_t, xo_t, h_tm1, c_tm1):\n",
        "        i_t = sigmoid(xi_t + T.dot(h_tm1, self.W_hi) + c_tm1 * self.W_ci)\n",
        "        f_t = sigmoid(xf_t + T.dot(h_tm1, self.W_hf) + c_tm1 * self.W_cf)\n",
        "        c_t = f_t * c_tm1 + i_t * tanh(xc_t + T.dot(h_tm1, self.W_hc))\n",
        "        o_t = sigmoid(xo_t + T.dot(h_tm1, self.W_ho) + c_t * self.W_co)\n",
        "        h_t = o_t * tanh(c_t)\n",
        "        return h_t, c_t\n",
        "\n",
        "    def forward(self, x, h0=None, mask=None):\n",
        "        xi = T.dot(x, self.W_xi) + self.b_xi\n",
        "        xf = T.dot(x, self.W_xf) + self.b_xf\n",
        "        xc = T.dot(x, self.W_xc) + self.b_xc\n",
        "        xo = T.dot(x, self.W_xo) + self.b_xo\n",
        "\n",
        "        inputs = [xi, xf, xc, xo]\n",
        "\n",
        "        if h0 is None:\n",
        "            h0 = T.zeros(shape=(x[0].shape[0], self.output_dim), dtype=theano.config.floatX)\n",
        "        c0 = T.zeros(shape=(x[0].shape[0], self.output_dim), dtype=theano.config.floatX)\n",
        "\n",
        "        [h, _], _ = theano.scan(fn=self._step,\n",
        "                                sequences=inputs,\n",
        "                                outputs_info=[h0, c0])\n",
        "        return h\n",
        "\n",
        "class StackLayer(object):\n",
        "    def __init__(self, name='StackLayer'):\n",
        "        self.name = name\n",
        "        self.layers = []\n",
        "        self.params = []\n",
        "\n",
        "    def _set_layers(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_rnn_unit(unit_type):\n",
        "        return LSTM\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_connect_unit(connect_type):\n",
        "        return Dense\n",
        "\n",
        "    def _set_params(self):\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            params.extend(layer.params)\n",
        "        return params\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class BiRNNLayer(StackLayer):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 unit_type,\n",
        "                 connect_type,\n",
        "                 drop_rate=0.0):\n",
        "        name = 'BiRNNs-%d:(%dx%d)' % (n_layers, input_dim, output_dim)\n",
        "        super(BiRNNLayer, self).__init__(name=name)\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn_unit = self._set_rnn_unit(unit_type)\n",
        "        self.connect_unit = self._set_connect_unit(connect_type)\n",
        "        self.dropout = Dropout(drop_rate)\n",
        "\n",
        "        self.layers = self._set_layers()\n",
        "        self.params = self._set_params()\n",
        "\n",
        "    def _set_layers(self):\n",
        "        layers = []\n",
        "        for i in range(self.n_layers):\n",
        "            if i == 0:\n",
        "                rnn_input_dim = self.input_dim\n",
        "                connect_input_dim = self.input_dim + self.output_dim\n",
        "            else:\n",
        "                rnn_input_dim = self.output_dim\n",
        "                connect_input_dim = self.output_dim * 2\n",
        "\n",
        "            r_unit = self.rnn_unit(input_dim=rnn_input_dim,\n",
        "                                   output_dim=self.output_dim)\n",
        "            c_unit = self.connect_unit(input_dim=connect_input_dim,\n",
        "                                       output_dim=self.output_dim,\n",
        "                                       activation='relu')\n",
        "            layers += [r_unit, c_unit]\n",
        "        return layers\n",
        "\n",
        "    def forward(self, x, mask=None, is_train=False):\n",
        "        n_layers = int(len(self.layers) / 2)\n",
        "        for i in range(n_layers):\n",
        "            if mask is None:\n",
        "                h = self.layers[i * 2].forward(x=x)\n",
        "                h = self.dropout.forward(x=h, is_train=is_train)\n",
        "                x = self.layers[i * 2 + 1].forward(T.concatenate([x, h], axis=2))\n",
        "            else:\n",
        "                h = self.layers[i * 2].forward(x=x, mask=mask)\n",
        "                h = self.dropout.forward(x=h, is_train=is_train)\n",
        "                x = self.layers[i * 2 + 1].forward(T.concatenate([x, h], axis=2)) * mask\n",
        "                mask = mask[::-1]\n",
        "            x = x[::-1]\n",
        "        if (n_layers % 2) == 1:\n",
        "            return x[::-1]\n",
        "        return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rT40zOxNyk7"
      },
      "source": [
        "class Regularizer(object):\n",
        "    def __call__(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class L2Regularizer(Regularizer):\n",
        "    def __call__(self, alpha, params):\n",
        "        return alpha * l2_sqr(params) / 2.\n",
        "\n",
        "def l2_sqr(params):\n",
        "    sqr = 0.0\n",
        "    for p in params:\n",
        "        sqr += T.sum((p ** 2))\n",
        "    return sqr\n",
        "\n",
        "def logsumexp3d(x, axis=2):\n",
        "    # 1D: batch_size, 2D: n_labels, 3D: 1\n",
        "    x_max = T.max(x, axis=axis, keepdims=True)\n",
        "    # 1D: batch_size, 2D: n_labels\n",
        "    return T.log(T.sum(T.exp(x - x_max), axis=axis)) + x_max.dimshuffle(0, 1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEnQNXfjt3hY"
      },
      "source": [
        "Span Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV8ncBR1Jazp"
      },
      "source": [
        "class Model(object):\n",
        "    def __init__(self):\n",
        "        self.is_train = theano.shared(0, borrow=True)\n",
        "        self.inputs = None\n",
        "        self.outputs = None\n",
        "        self.dropout = None\n",
        "        self.input_layers = []\n",
        "        self.hidden_layers = []\n",
        "        self.output_layers = []\n",
        "        self.layers = []\n",
        "        self.params = []\n",
        "\n",
        "    def compile(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _set_params(self):\n",
        "        for l in self.layers:\n",
        "            self.params += l.params\n",
        "\n",
        "class FeatureLayer(Model):\n",
        "    def compile(self, **kwargs):\n",
        "        self._set_layers(kwargs)\n",
        "        self._set_params()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embs = []\n",
        "        for i in range(len(inputs)):\n",
        "            # 1D: batch_size, 2D: n_words, 3D: input_dim\n",
        "            emb_i = self.input_layers[i].forward(x=inputs[i],\n",
        "                                                 is_train=self.is_train)\n",
        "            embs.append(emb_i)\n",
        "\n",
        "        # 1D: batch_size, 2D: n_words, 3D: input_dim\n",
        "        x = T.concatenate(tensor_list=embs, axis=2)\n",
        "        # 1D: n_words, 2D: batch_size, 3D: hidden_dim\n",
        "        h = self.hidden_layers[0].forward(x=x.dimshuffle(1, 0, 2),\n",
        "                                          is_train=self.is_train)\n",
        "        return h\n",
        "\n",
        "    def _set_layers(self, args):\n",
        "        x_w_dim, x_m_dim = args['input_dim']\n",
        "        hidden_dim = args['hidden_dim']\n",
        "        drop_rate = args['drop_rate']\n",
        "\n",
        "        if args['vocab_word_size'] > 0:\n",
        "            emb_word = Embedding(input_dim=args['vocab_word_size'],\n",
        "                                 output_dim=x_w_dim,\n",
        "                                 init_emb=args['word_emb'],\n",
        "                                 param_fix=True,\n",
        "                                 drop_rate=drop_rate,\n",
        "                                 name='EmbWord')\n",
        "            self.input_layers.append(emb_word)\n",
        "\n",
        "        if args['use_elmo']:\n",
        "            emb_elmo = ElmoLayer(drop_rate=0.5,\n",
        "                                 name='EmbElmo')\n",
        "            self.input_layers.append(emb_elmo)\n",
        "\n",
        "        emb_mark = Embedding(input_dim=2,\n",
        "                             output_dim=x_m_dim,\n",
        "                             init_emb=None,\n",
        "                             param_init='xavier',\n",
        "                             param_fix=False,\n",
        "                             drop_rate=drop_rate,\n",
        "                             name='EmbMark')\n",
        "        self.input_layers.append(emb_mark)\n",
        "\n",
        "        if args['use_elmo']:\n",
        "            hidden_input_dim = (len(self.input_layers) - 2) * x_w_dim + x_m_dim + 1024\n",
        "        else:\n",
        "            hidden_input_dim = (len(self.input_layers) - 1) * x_w_dim + x_m_dim\n",
        "        hidden_layer = BiRNNLayer(input_dim=hidden_input_dim,\n",
        "                                  output_dim=hidden_dim,\n",
        "                                  n_layers=args['n_layers'],\n",
        "                                  unit_type='lstm',\n",
        "                                  connect_type='dense',\n",
        "                                  drop_rate=drop_rate)\n",
        "        self.hidden_layers = [hidden_layer]\n",
        "        self.layers = self.input_layers + self.hidden_layers\n",
        "\n",
        "class LabelLayer(Model):\n",
        "    def compile(self, **kwargs):\n",
        "        self._set_layers(hidden_dim=kwargs['feat_dim'],\n",
        "                         output_dim=kwargs['output_dim'])\n",
        "        self._set_params()\n",
        "\n",
        "    def _set_layers(self, hidden_dim, output_dim):\n",
        "        self.layers = [Dense(input_dim=hidden_dim,\n",
        "                             output_dim=output_dim)]\n",
        "\n",
        "    def span_feats2(self, h):\n",
        "        \"\"\"\n",
        "        :param h: 1D: n_words, 2D: batch_size, 3D: hidden_dim\n",
        "        :return: 1D: batch_size, 2D: n_spans, 3D: 2 * hidden_dim\n",
        "        \"\"\"\n",
        "        h = h.dimshuffle(1, 0, 2)\n",
        "        n_words = h.shape[1]\n",
        "\n",
        "        m = T.triu(T.ones(shape=(n_words, n_words)))\n",
        "        indices = m.nonzero()\n",
        "\n",
        "        # 1D: batch_size, 2D: n_spans, 3D: hidden_dim\n",
        "        h_i = h[:, indices[0]]\n",
        "        h_j = h[:, indices[1]]\n",
        "\n",
        "        h_diff = h_i - h_j\n",
        "        h_add = h_i + h_j\n",
        "\n",
        "        return T.concatenate([h_add, h_diff], axis=2)\n",
        "\n",
        "    def span_feats(self, h):\n",
        "        h = h.dimshuffle(1, 0, 2)\n",
        "        n_words = h.shape[1]\n",
        "        pad = T.zeros(shape=(h.shape[0], 1, h.shape[2]))\n",
        "        h_pad = T.concatenate([h, pad], axis=1)\n",
        "\n",
        "        m = T.triu(T.ones(shape=(n_words, n_words)))\n",
        "        indices = m.nonzero()\n",
        "\n",
        "        # 1D: batch_size, 2D: n_spans, 3D: hidden_dim\n",
        "        h_i = h[:, indices[0]]\n",
        "        h_j = h_pad[:, indices[1] + 1]\n",
        "\n",
        "        h_diff = h_i - h_j\n",
        "        h_add = h_i + h_j\n",
        "\n",
        "        return T.concatenate([h_add, h_diff], axis=2)\n",
        "\n",
        "    def logit_scores(self, h):\n",
        "        return self.layers[-1].forward(h).dimshuffle(0, 2, 1)\n",
        "\n",
        "class SpanModel(Model):\n",
        "    def __init__(self):\n",
        "        super(SpanModel, self).__init__()\n",
        "        self.feat_layer = None\n",
        "        self.label_layer = None\n",
        "\n",
        "    def compile(self, inputs, **kwargs):\n",
        "        self.inputs = inputs\n",
        "        self.feat_layer = FeatureLayer()\n",
        "        self.feat_layer.compile(**kwargs)\n",
        "        self.label_layer = LabelLayer()\n",
        "        self.label_layer.compile(**kwargs)\n",
        "        self.layers = self.feat_layer.layers + self.label_layer.layers\n",
        "        self._set_params()\n",
        "\n",
        "    def span_feats(self, inputs):\n",
        "        \"\"\"\n",
        "        :param inputs: 1D: n_inputs, 2D: batch_size, 3D: n_words; feat id\n",
        "        :return: 1D: batch_size, 2D: n_spans, 3D: 2 * hidden_dim\n",
        "        \"\"\"\n",
        "        # 1D: n_words, 2D: batch_size, 3D: 2 * hidden_dim\n",
        "        h_rnn = self.feat_layer.forward(inputs)\n",
        "        return self.label_layer.span_feats(h_rnn)\n",
        "\n",
        "    @staticmethod\n",
        "    def argmax_span(span_score):\n",
        "        \"\"\"\n",
        "        :param span_score: 1D: batch_size, 2D: n_labels, 3D: n_spans\n",
        "        :return: 1D: batch_size, 2D: n_labels; span index\n",
        "        \"\"\"\n",
        "        return T.argmax(span_score, axis=2)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(span_score, span_true):\n",
        "        \"\"\"\n",
        "        :param span_score: 1D: batch_size, 2D: n_labels, 3D: n_spans\n",
        "        :param span_true: 1D: batch_size * n_spans; (batch index, label id, span index)\n",
        "        \"\"\"\n",
        "        batch_size = span_score.shape[0]\n",
        "\n",
        "        # 1D: batch_size * n_spans; index\n",
        "        batch_index = span_true[:, 0]\n",
        "        label_index = span_true[:, 1]\n",
        "        span_index = span_true[:, 2]\n",
        "\n",
        "        # 1D: batch_size * n_spans; score\n",
        "        true_span_score = span_score[batch_index, label_index, span_index]\n",
        "\n",
        "        # 1D: batch_size, 2D: n_labels; elem=score\n",
        "        z = logsumexp3d(span_score, axis=2)\n",
        "        # 1D: batch_size * n_spans; score\n",
        "        z = z[batch_index, label_index]\n",
        "\n",
        "        # 1D: batch_size * n_spans; score\n",
        "        nll = true_span_score - z\n",
        "\n",
        "        return - T.sum(nll) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def exp_score(span_score):\n",
        "        \"\"\"\n",
        "        :param span_score: 1D: batch_size, 2D: n_labels, 3D: n_spans; logit score\n",
        "        :return: 1D: batch_size, 2D: n_labels, 3D: n_spans\n",
        "        \"\"\"\n",
        "        return T.exp(span_score)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB226u1qfMN6"
      },
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "def save_pickle(fn, data):\n",
        "    with gzip.open(fn + '.pkl.gz', 'wb') as gf:\n",
        "        pickle.dump(data, gf, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_pickle(fn):\n",
        "    with gzip.open(fn, 'rb') as gf:\n",
        "        return pickle.load(gf)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cec-266Qt7j0"
      },
      "source": [
        "Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISA-yKQdfBBS"
      },
      "source": [
        "def get_optimizer(argv):\n",
        "    if argv.opt_type == 'adam':\n",
        "        return Adam(argv=argv, lr=argv.lr, grad_clip=argv.grad_clip)\n",
        "    return SGD(argv=argv, lr=argv.lr, grad_clip=argv.grad_clip)\n",
        "\n",
        "class Optimizer(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.argv = kwargs['argv']\n",
        "        self.grad_clip = kwargs['grad_clip']\n",
        "        self.params = []\n",
        "\n",
        "    def __call__(self, grads, params):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def set_params(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def init_params(self):\n",
        "        for p in self.params:\n",
        "            p.set_value(p.get_value(borrow=True) * 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _grad_clipping(gradients, max_norm=5.0):\n",
        "        global_grad_norm = T.sqrt(sum(map(lambda x: T.sqr(x).sum(), gradients)))\n",
        "        multiplier = T.switch(global_grad_norm < max_norm, 1.0, max_norm / global_grad_norm)\n",
        "        return [g * multiplier for g in gradients]\n",
        "\n",
        "    def save_params(self, epoch=0):\n",
        "        argv = self.argv\n",
        "        if argv.output_dir:\n",
        "            dir_name = argv.output_dir\n",
        "        else:\n",
        "            dir_name = 'output'\n",
        "        if argv.output_fn:\n",
        "            file_name = '/opt.param.%s.epoch-%d' % (argv.output_fn, epoch)\n",
        "        else:\n",
        "            file_name = '/opt.param.%s.epoch-%d' % (argv.method, epoch)\n",
        "\n",
        "        fn = dir_name + file_name\n",
        "        params = [p.get_value(borrow=True) for p in self.params]\n",
        "        save_pickle(fn=fn, data=params)\n",
        "\n",
        "    def load_params(self, path):\n",
        "        params = load_pickle(path)\n",
        "        assert len(self.params) == len(params)\n",
        "        for p1, p2 in zip(self.params, params):\n",
        "            p1.set_value(p2)\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, lr=0.001, **kwargs):\n",
        "        super(SGD, self).__init__(**kwargs)\n",
        "        self.lr = theano.shared(np.asarray(lr, dtype=theano.config.floatX), borrow=True)\n",
        "\n",
        "    def __call__(self, params, grads):\n",
        "        updates = []\n",
        "        if self.grad_clip:\n",
        "            grads = self._grad_clipping(grads, max_norm=1.0)\n",
        "        for p, g in zip(params, grads):\n",
        "            updates.append((p, p - self.lr * g))\n",
        "        return updates\n",
        "\n",
        "    def set_params(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Adam(Optimizer):\n",
        "    def __init__(self, lr=0.001, b1=0.9, b2=0.999, eps=1e-8, **kwargs):\n",
        "        super(Adam, self).__init__(**kwargs)\n",
        "        self.lr = theano.shared(np.asarray(lr, dtype=theano.config.floatX), borrow=True)\n",
        "        self.b1 = b1\n",
        "        self.b2 = b2\n",
        "        self.eps = eps\n",
        "\n",
        "    def __call__(self, params, grads):\n",
        "        updates = []\n",
        "\n",
        "        i = self.params[0]\n",
        "        i_t = i + 1.\n",
        "        a_t = self.lr * T.sqrt(1 - self.b2 ** i_t) / (1 - self.b1 ** i_t)\n",
        "\n",
        "        if self.grad_clip:\n",
        "            grads = self._grad_clipping(grads, max_norm=1.0)\n",
        "\n",
        "        for index, (p, g) in enumerate(zip(params, grads)):\n",
        "            v = self.params[2 * index + 1]\n",
        "            r = self.params[2 * index + 2]\n",
        "            index += 2\n",
        "\n",
        "            v_t = self.b1 * v + (1. - self.b1) * g\n",
        "            r_t = self.b2 * r + (1. - self.b2) * g ** 2\n",
        "\n",
        "            step = a_t * v_t / (T.sqrt(r_t) + self.eps)\n",
        "\n",
        "            updates.append((v, v_t))\n",
        "            updates.append((r, r_t))\n",
        "            updates.append((p, p - step))\n",
        "\n",
        "        updates.append((i, i_t))\n",
        "        return updates\n",
        "\n",
        "    def set_params(self, params):\n",
        "        i = theano.shared(np.asarray(.0, dtype=theano.config.floatX))\n",
        "        self.params.append(i)\n",
        "        for p in params:\n",
        "            p_tm = p.get_value(borrow=True)\n",
        "            v = theano.shared(np.zeros(p_tm.shape, dtype=p_tm.dtype))\n",
        "            r = theano.shared(np.zeros(p_tm.shape, dtype=p_tm.dtype))\n",
        "            self.params += [v, r]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SWKmVZJ1EY0"
      },
      "source": [
        "def correct_and_pred_spans(span_true, span_pred, marks):\n",
        "    \n",
        "    correct = 0.\n",
        "    n_pred_spans = 0.\n",
        "    n_words = len(marks[0])\n",
        "    _, prd_indices = np.array(marks).nonzero()\n",
        "    prd_indices = [span_to_span_index(p, p, n_words) for p in prd_indices]\n",
        "\n",
        "    for b_index, span_pred_tmp in enumerate(span_pred):\n",
        "        prd_index = prd_indices[b_index]\n",
        "        for label_id, span_index in enumerate(span_pred_tmp):\n",
        "            if span_index == prd_index:\n",
        "                continue\n",
        "            if [b_index, label_id, span_index] in span_true:\n",
        "                correct += 1\n",
        "            n_pred_spans += 1\n",
        "\n",
        "    return correct, n_pred_spans\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTzJEAbM8dUK"
      },
      "source": [
        "\n",
        "class SpanModelAPI(object):\n",
        "    def __init__(self, argv):\n",
        "        self.argv = argv\n",
        "\n",
        "        self.model = None\n",
        "        self.experts = None\n",
        "        self.train_func = None\n",
        "        self.pred_func = None\n",
        "\n",
        "        self.vocab_word = None\n",
        "        self.vocab_label = None\n",
        "        self.vocab_label_valid = None\n",
        "\n",
        "        self.input_dim = None\n",
        "        self.hidden_dim = None\n",
        "        self.output_dim = None\n",
        "        self.use_elmo = None\n",
        "\n",
        "        self.decoder = None\n",
        "        self.optimizer = None\n",
        "\n",
        "        self.n_true_spans = 0.\n",
        "\n",
        "    def set_model(self, **kwargs):\n",
        "        write('Setting a model...')\n",
        "        argv = self.argv\n",
        "\n",
        "        self.vocab_word = kwargs['vocab_word']\n",
        "        self.use_elmo = kwargs['use_elmo']\n",
        "        self.vocab_label = kwargs['vocab_label']\n",
        "        self.vocab_label_valid = kwargs['vocab_label_valid']\n",
        "        word_emb = kwargs['word_emb']\n",
        "        vocab_word_size = self.vocab_word.size() if self.vocab_word else 0\n",
        "\n",
        "        self.input_dim = argv.emb_dim if word_emb is None else word_emb.shape[1]\n",
        "        self.hidden_dim = argv.hidden_dim\n",
        "        self.output_dim = -1\n",
        "\n",
        "        self.decoder = Decoder(argv=argv, vocab_label=self.vocab_label)\n",
        "\n",
        "        self.model = SpanModel()\n",
        "        self.model.compile(inputs=self._set_inputs(),\n",
        "                           vocab_word_size=vocab_word_size,\n",
        "                           use_elmo=self.use_elmo,\n",
        "                           word_emb=word_emb,\n",
        "                           input_dim=[self.input_dim, self.input_dim],\n",
        "                           hidden_dim=self.hidden_dim,\n",
        "                           feat_dim=2 * self.hidden_dim,\n",
        "                           output_dim=self.vocab_label.size(),\n",
        "                           n_layers=argv.n_layers,\n",
        "                           drop_rate=argv.drop_rate)\n",
        "\n",
        "        write('\\t- {}'.format(\"\\n\\t- \".join([l.name for l in self.model.layers])))\n",
        "        self._show_model_config()\n",
        "\n",
        "    def set_ensemble_model(self, **kwargs):\n",
        "        write('Setting a model...')\n",
        "        argv = self.argv\n",
        "\n",
        "        self.vocab_word = kwargs['vocab_word']\n",
        "        self.use_elmo = kwargs['use_elmo']\n",
        "        self.vocab_label = kwargs['vocab_label']\n",
        "        self.vocab_label_valid = kwargs['vocab_label_valid']\n",
        "        word_emb = kwargs['word_emb']\n",
        "        vocab_word_size = self.vocab_word.size() if self.vocab_word else 0\n",
        "\n",
        "        self.input_dim = argv.emb_dim if word_emb is None else word_emb.shape[1]\n",
        "        self.hidden_dim = argv.hidden_dim\n",
        "        self.output_dim = -1\n",
        "\n",
        "        self.decoder = Decoder(argv=argv, vocab_label=self.vocab_label)\n",
        "\n",
        "        \n",
        "        inputs = self._set_inputs()\n",
        "        self.model = MoEModel()\n",
        "        self.model.compile(inputs=inputs,\n",
        "                           feat_dim=2 * self.hidden_dim,\n",
        "                           output_dim=self.vocab_label.size(),\n",
        "                           drop_rate=argv.drop_rate,\n",
        "                           n_experts=argv.n_experts)\n",
        "        write('\\t- {}\\n'.format(\"\\n\\t- \".join([l.name for l in self.model.layers])))\n",
        "\n",
        "        \n",
        "        experts = []\n",
        "        for _ in range(argv.n_experts):\n",
        "            model = SpanModel()\n",
        "            model.compile(inputs=self.model.inputs,\n",
        "                          vocab_word_size=vocab_word_size,\n",
        "                          use_elmo=self.use_elmo,\n",
        "                          input_dim=[self.input_dim, self.input_dim],\n",
        "                          hidden_dim=self.hidden_dim,\n",
        "                          feat_dim=2 * self.hidden_dim,\n",
        "                          output_dim=self.vocab_label.size(),\n",
        "                          n_layers=argv.n_layers,\n",
        "                          word_emb=word_emb,\n",
        "                          drop_rate=argv.drop_rate)\n",
        "            write('\\t- {}\\n'.format(\"\\n\\t- \".join([l.name for l in model.layers])))\n",
        "            experts.append(model)\n",
        "\n",
        "        self.experts = experts\n",
        "\n",
        "    def _set_inputs(self):\n",
        "        x = []\n",
        "        if self.vocab_word:\n",
        "            x.append(T.imatrix('x_word'))\n",
        "        if self.use_elmo:\n",
        "            x.append(T.ftensor4('x_elmo'))\n",
        "        x.append(T.imatrix('x_mark'))\n",
        "        assert len(x) > 1\n",
        "        return x\n",
        "\n",
        "    def _show_model_config(self):\n",
        "        model = self.model\n",
        "        write('Model configuration')\n",
        "        write('\\t- Input  Dim: {}'.format(self.input_dim))\n",
        "        write('\\t- Hidden Dim: {}'.format(self.hidden_dim))\n",
        "        write('\\t- Output Dim: {}'.format(self.output_dim))\n",
        "        write('\\t- Parameters: {}'.format(sum(len(x.get_value(borrow=True).ravel())\n",
        "                                              for x in model.params)))\n",
        "\n",
        "    def save_params(self, epoch=-1):\n",
        "        argv = self.argv\n",
        "        if argv.output_dir:\n",
        "            dir_name = argv.output_dir\n",
        "        else:\n",
        "            dir_name = 'output'\n",
        "        if argv.output_fn:\n",
        "            file_name = '/param.%s.epoch-%d' % (argv.output_fn, epoch)\n",
        "        else:\n",
        "            file_name = '/param.epoch-%d' % epoch\n",
        "\n",
        "        fn = dir_name + file_name\n",
        "        params = [p.get_value(borrow=True) for p in self.model.params]\n",
        "        save_pickle(fn=fn, data=params)\n",
        "\n",
        "    def load_params(self, path):\n",
        "        params = load_pickle(path)\n",
        "        assert len(self.model.params) == len(params)\n",
        "        for p1, p2 in zip(self.model.params, params):\n",
        "            p1.set_value(p2)\n",
        "\n",
        "    def load_experts_params(self, path):\n",
        "        write('Loading experts params...')\n",
        "        param_files = glob.glob(path + '/*')\n",
        "        param_files = [fn for fn in param_files\n",
        "                       if fn.split('/')[-1].startswith('param')]\n",
        "        write(\"\\t - Param Files: %s\" % str(param_files))\n",
        "        for i, path in enumerate(param_files[:self.argv.n_experts]):\n",
        "            params = load_pickle(path)\n",
        "            assert len(self.experts[i].params) == len(params)\n",
        "            for p1, p2 in zip(self.experts[i].params, params):\n",
        "                p1.set_value(p2)\n",
        "\n",
        "    def set_init_ensemble_param(self):\n",
        "        write('Initializing params...')\n",
        "        W = np.zeros(shape=(2 * self.hidden_dim, self.vocab_label.size()),\n",
        "                     dtype=theano.config.floatX)\n",
        "        b = np.zeros(shape=self.vocab_label.size(),\n",
        "                     dtype=theano.config.floatX)\n",
        "        for model in self.experts:\n",
        "            W += model.params[-2].get_value(borrow=True)\n",
        "        for model in self.experts:\n",
        "            b += model.params[-1].get_value(borrow=True)\n",
        "        W = W / len(self.experts)\n",
        "        b = b / len(self.experts)\n",
        "        self.model.params[-2].set_value(W)\n",
        "        self.model.params[-1].set_value(b)\n",
        "\n",
        "    def set_train_func(self):\n",
        "        write('Building a training function...')\n",
        "\n",
        "        self.optimizer = get_optimizer(self.argv)\n",
        "        self.optimizer.set_params(self.model.params)\n",
        "        if self.argv.load_opt_param:\n",
        "            self.optimizer.load_params(self.argv.load_opt_param)\n",
        "\n",
        "        span_true = T.imatrix('span_true')\n",
        "\n",
        "        h_span = self.model.span_feats(inputs=self.model.inputs)\n",
        "        span_score = self.model.label_layer.logit_scores(h=h_span)\n",
        "        span_pred = self.model.argmax_span(span_score=span_score)\n",
        "\n",
        "        nll = self.model.loss(span_score, span_true)\n",
        "        l2_reg = L2Regularizer()\n",
        "        objective = nll + l2_reg(alpha=self.argv.reg,\n",
        "                                 params=self.model.params)\n",
        "\n",
        "        grads = T.grad(cost=objective, wrt=self.model.params)\n",
        "        updates = self.optimizer(grads=grads, params=self.model.params)\n",
        "\n",
        "        self.train_func = theano.function(\n",
        "            inputs=self.model.inputs + [span_true],\n",
        "            outputs=[objective, span_pred],\n",
        "            updates=updates,\n",
        "            mode='FAST_RUN'\n",
        "        )\n",
        "\n",
        "    def set_pred_func(self):\n",
        "        write('Building a predicting function...')\n",
        "        if self.argv.search == 'argmax':\n",
        "            self.set_pred_argmax_func()\n",
        "        else:\n",
        "            self.set_pred_score_func()\n",
        "\n",
        "    def set_pred_argmax_func(self):\n",
        "        h_span = self.model.span_feats(inputs=self.model.inputs)\n",
        "        logits = self.model.label_layer.logit_scores(h_span)\n",
        "        span_pred = self.model.argmax_span(logits)\n",
        "\n",
        "        self.pred_func = theano.function(\n",
        "            inputs=self.model.inputs,\n",
        "            outputs=span_pred,\n",
        "            mode='FAST_RUN'\n",
        "        )\n",
        "\n",
        "    def set_pred_score_func(self):\n",
        "        h_span = self.model.span_feats(inputs=self.model.inputs)\n",
        "        logits = self.model.label_layer.logit_scores(h_span)\n",
        "        span_score = self.model.exp_score(logits)\n",
        "\n",
        "        self.pred_func = theano.function(\n",
        "            inputs=self.model.inputs,\n",
        "            outputs=span_score,\n",
        "            mode='FAST_RUN'\n",
        "        )\n",
        "\n",
        "    def set_ensemble_train_func(self):\n",
        "        write('Building an ensemble training function...')\n",
        "\n",
        "        self.optimizer = get_optimizer(self.argv)\n",
        "        self.optimizer.set_params(self.model.params)\n",
        "        if self.argv.load_opt_param:\n",
        "            self.optimizer.load_params(self.argv.load_opt_param)\n",
        "\n",
        "        span_true = T.imatrix('span_true')\n",
        "\n",
        "        h_span = self.model.feat_layer.forward(self.model.inputs,\n",
        "                                               self.experts)\n",
        "        logits = self.model.feat_layer.logit_scores(h=h_span)\n",
        "        span_pred = self.model.argmax_span(logits)\n",
        "\n",
        "        nll = self.model.loss(logits, span_true)\n",
        "        l2_reg = L2Regularizer()\n",
        "        objective = nll + l2_reg(alpha=self.argv.reg,\n",
        "                                 params=self.model.params)\n",
        "\n",
        "        grads = T.grad(cost=objective, wrt=self.model.params)\n",
        "        updates = self.optimizer(grads=grads,\n",
        "                                 params=self.model.params)\n",
        "\n",
        "        self.train_func = theano.function(\n",
        "            inputs=self.model.inputs + [span_true],\n",
        "            outputs=[objective, span_pred],\n",
        "            updates=updates,\n",
        "            mode='FAST_RUN'\n",
        "        )\n",
        "\n",
        "    def set_ensemble_pred_func(self):\n",
        "        write('Building an ensemble predicting function...')\n",
        "        if self.argv.search == 'argmax':\n",
        "            self.set_ensemble_pred_argmax_func()\n",
        "        else:\n",
        "            self.set_ensemble_pred_score_func()\n",
        "\n",
        "    def set_ensemble_pred_argmax_func(self):\n",
        "        # 1D: batch_size, 2D: n_spans, 3D: 2 * hidden_dim\n",
        "        h_span = self.model.feat_layer.forward(self.model.inputs,\n",
        "                                               self.experts)\n",
        "        # 1D: batch_size, 2D: n_labels, 3D: n_spans; score\n",
        "        span_score = self.model.feat_layer.logit_scores(h=h_span)\n",
        "        # 1D: batch_size, 2D: n_labels; span index\n",
        "        span_pred = self.model.argmax_span(span_score=span_score)\n",
        "\n",
        "        self.pred_func = theano.function(\n",
        "            inputs=self.model.inputs,\n",
        "            outputs=span_pred,\n",
        "            mode='FAST_RUN'\n",
        "        )\n",
        "\n",
        "    def set_ensemble_pred_score_func(self):\n",
        "        # 1D: batch_size, 2D: n_spans, 3D: 2 * hidden_dim\n",
        "        h_span = self.model.feat_layer.forward(self.model.inputs,\n",
        "                                               self.experts)\n",
        "        # 1D: batch_size, 2D: n_labels, 3D: n_spans; score\n",
        "        logits = self.model.feat_layer.logit_scores(h=h_span)\n",
        "        # 1D: batch_size, 2D: n_labels, 3D: n_spans; score\n",
        "        span_score = self.model.exp_score(logits)\n",
        "\n",
        "        self.pred_func = theano.function(\n",
        "            inputs=self.model.inputs,\n",
        "            outputs=span_score,\n",
        "            mode='FAST_RUN'\n",
        "        )\n",
        "\n",
        "    def train(self, batches):\n",
        "        start = time.time()\n",
        "        n_batches = 0.\n",
        "        loss_total = 0.\n",
        "        p_total = 0.\n",
        "        correct = 0.\n",
        "\n",
        "        self.model.feat_layer.is_train.set_value(1)\n",
        "        if self.experts:\n",
        "            for model in self.experts:\n",
        "                model.feat_layer.is_train.set_value(1)\n",
        "\n",
        "        for inputs in batches:\n",
        "            n_batches += 1\n",
        "\n",
        "            if n_batches % 100 == 0:\n",
        "                sys.stdout.write(\"%d \" % n_batches)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            n_words = len(inputs[0][0])\n",
        "            if n_words < 2 or 100 < n_words:\n",
        "                continue\n",
        "\n",
        "            loss, span_pred = self.train_func(*inputs)\n",
        "\n",
        "            if math.isnan(loss):\n",
        "                write('\\n\\nNAN: Index: %d\\n' % n_batches)\n",
        "                exit()\n",
        "\n",
        "            loss_total += loss\n",
        "            correct_i, p_total_i = correct_and_pred_spans(span_true=inputs[-1],\n",
        "                                                          span_pred=span_pred,\n",
        "                                                          marks=inputs[1])\n",
        "            correct += correct_i\n",
        "            p_total += p_total_i\n",
        "\n",
        "        self.model.feat_layer.is_train.set_value(0)\n",
        "        if self.experts:\n",
        "            for model in self.experts:\n",
        "                model.feat_layer.is_train.set_value(0)\n",
        "\n",
        "        avg_loss = loss_total / n_batches\n",
        "        p, r, f = f_score(correct, p_total, self.n_true_spans)\n",
        "\n",
        "        write('\\n\\tTime: %f seconds' % (time.time() - start))\n",
        "        write('\\tAverage Negative Log Likelihood: %f(%f/%d)' % (avg_loss, loss_total, n_batches))\n",
        "        write('\\tF:{:>7.2%}  P:{:>7.2%} ({:>5}/{:>5})  R:{:>7.2%} ({:>5}/{:>5})'.format(\n",
        "            f, p, int(correct), int(p_total), r, int(correct), int(self.n_true_spans)))\n",
        "\n",
        "    def predict(self, batches):\n",
        "        if self.argv.search == 'argmax':\n",
        "            return self.predict_argmax(batches)\n",
        "        else:\n",
        "            return self.predict_greedy(batches)\n",
        "\n",
        "    def predict_argmax(self, batches):\n",
        "        \"\"\"\n",
        "        :param batches: 1D: n_sents, 2D: n_prds, 3D: n_feats, 4D: n_words; elem=(x_w, x_m)\n",
        "        :return: y: 1D: n_sents, 2D: n_prds, 3D: n_spans, 3D: [label_id, pre_index, post_index]\n",
        "        \"\"\"\n",
        "        start = time.time()\n",
        "        y = []\n",
        "\n",
        "        for index, inputs in enumerate(batches):\n",
        "            if (index + 1) % 100 == 0:\n",
        "                sys.stdout.write(\"%d \" % (index + 1))\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            if len(inputs) == 0:\n",
        "                span_triples = []\n",
        "            else:\n",
        "                span_pred = self.pred_func(*inputs)\n",
        "                span_triples = self.decoder.argmax_span_triples(span_indices=span_pred,\n",
        "                                                                marks=inputs[-1])\n",
        "            y.append(span_triples)\n",
        "\n",
        "        write('\\n\\tTime: %f seconds' % (time.time() - start))\n",
        "        return y\n",
        "\n",
        "    def predict_greedy(self, batches):\n",
        "        \"\"\"\n",
        "        :param batches: 1D: n_sents, 2D: n_prds, 3D: n_feats, 4D: n_words; elem=(x_w, x_m)\n",
        "        :return: y: 1D: n_sents, 2D: n_prds, 3D: n_spans, 3D: [label_id, pre_index, post_index]\n",
        "        \"\"\"\n",
        "        start = time.time()\n",
        "        y = []\n",
        "\n",
        "        for index, inputs in enumerate(batches):\n",
        "            if (index + 1) % 100 == 0:\n",
        "                sys.stdout.write(\"%d \" % (index + 1))\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            if len(inputs) == 0:\n",
        "                span_triples = []\n",
        "            else:\n",
        "                scores = self.pred_func(*inputs)\n",
        "                span_triples = self.decoder.greedy_span_triples(scores=scores,\n",
        "                                                                marks=inputs[-1])\n",
        "            y.append(span_triples)\n",
        "\n",
        "        write('\\n\\tTime: %f seconds' % (time.time() - start))\n",
        "        return y"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI6vjCemQnV2"
      },
      "source": [
        "def count_true_spans(sents):\n",
        "    \"\"\"\n",
        "    :param sents: 1D: n_sents\n",
        "    :return: total number of spans\n",
        "    \"\"\"\n",
        "    return sum([len(triple) for sent in sents for triple in sent.span_triples])\n",
        "\n",
        "def load_emb(path):\n",
        "    word_list = []\n",
        "    emb = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip().split()\n",
        "            word_list.append(line[0])\n",
        "            emb.append(line[1:])\n",
        "    emb = np.asarray(emb, dtype=theano.config.floatX)\n",
        "\n",
        "    if UNK not in word_list:\n",
        "        word_list = [UNK] + word_list\n",
        "        unk_vector = np.mean(emb, axis=0)\n",
        "        emb = np.vstack((unk_vector, emb))\n",
        "\n",
        "    return word_list, emb\n",
        "\n",
        "def write(s, stream=sys.stdout):\n",
        "    stream.write(s + '\\n')\n",
        "    stream.flush()\n",
        "\n",
        "\n",
        "def show_score_history(history, memo=''):\n",
        "    write('F1 HISTORY' + memo)\n",
        "    for k, v in sorted(history.items()):\n",
        "        epoch_tm = '\\t- EPOCH-{:d}  '.format(k)\n",
        "        if len(v) == 1:\n",
        "            f1_valid = '\\tBEST VALID {:>7.2%}'.format(v[0])\n",
        "            write(epoch_tm + f1_valid)\n",
        "        else:\n",
        "            v1, v2 = v\n",
        "            f1_valid = '\\tBEST VALID {:>7.2%}'.format(v1)\n",
        "            f1_evalu = '\\tEVALU {:>7.2%}'.format(v2)\n",
        "            write(epoch_tm + f1_valid + f1_evalu)\n",
        "\n",
        "def str_to_id(sent, vocab, unk):\n",
        "    \"\"\"\n",
        "    :param sent: 1D: n_words\n",
        "    :param vocab: Vocab()\n",
        "    :return: 1D: n_words; elem=id\n",
        "    \"\"\"\n",
        "    return list(map(lambda w: vocab.get_id(w) if vocab.has_key(w) else vocab.get_id(unk), sent))\n",
        "\n",
        "\n",
        "def make_vocab_from_ids(key_value_format):\n",
        "    vocab = Vocab()\n",
        "    for key, value in key_value_format:\n",
        "        vocab.add_word(key)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def array(sample, is_float=False):\n",
        "    if is_float:\n",
        "        return np.asarray(sample, dtype=theano.config.floatX)\n",
        "    return np.asarray(sample, dtype='int32')\n",
        "\n",
        "\n",
        "def average_vector(emb):\n",
        "    return np.mean(np.asarray(emb[2:], dtype=theano.config.floatX), axis=0)\n",
        "\n",
        "\n",
        "def unit_vector(vecs, axis):\n",
        "    return vecs / np.sqrt(np.sum(vecs ** 2, axis=axis, keepdims=True))\n",
        "\n",
        "\n",
        "def make_output_dir(argv):\n",
        "    if argv.output_dir:\n",
        "        output_dir = argv.output_dir\n",
        "    else:\n",
        "        output_dir = 'output'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def join_dir_and_file_names(dir_name, file_name):\n",
        "    return os.path.join(dir_name, file_name)\n",
        "\n",
        "\n",
        "def get_file_names_in_dir(dir_path, prefix=None, suffix=None):\n",
        "    file_names = glob.glob(dir_path + '/*')\n",
        "    if prefix:\n",
        "        file_names = [fn for fn in file_names\n",
        "                      if os.path.basename(fn).startswith(prefix)]\n",
        "    if suffix:\n",
        "        file_names = [fn for fn in file_names\n",
        "                      if fn.endswith(suffix)]\n",
        "    return file_names\n",
        "\n",
        "\n",
        "def get_latest_param_fn(file_names):\n",
        "    latest_epoch = -1\n",
        "    latest_fn = None\n",
        "    for fn in file_names:\n",
        "        for elem in fn.split('.'):\n",
        "            if elem.startswith('epoch'):\n",
        "                epoch = int(elem[6:])\n",
        "                if latest_epoch < epoch:\n",
        "                    latest_epoch = epoch\n",
        "                    latest_fn = fn\n",
        "                    break\n",
        "    assert latest_fn is not None\n",
        "    return latest_fn, latest_epoch\n",
        "\n",
        "\n",
        "def span_to_span_index(i, j, n_words):\n",
        "    return i * (n_words - 1) + j - np.arange(i).sum()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-qNgQpQuWF8"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5GbLB3DQKrM"
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self,\n",
        "                 argv,\n",
        "                 loader,\n",
        "                 preprocessor,\n",
        "                 evaluator,\n",
        "                 model_api):\n",
        "        self.argv = argv\n",
        "        self.loader = loader\n",
        "        self.preprocessor = preprocessor\n",
        "        self.evaluator = evaluator\n",
        "        self.model_api = model_api\n",
        "\n",
        "        self.f1_history = {}\n",
        "        self.best_valid_f1 = 0.0\n",
        "        self.best_epoch = -1\n",
        "\n",
        "    def train(self):\n",
        "        write('\\nTRAINING START\\n')\n",
        "\n",
        "        argv = self.argv\n",
        "        loader = self.loader\n",
        "        pproc = self.preprocessor\n",
        "\n",
        "        make_output_dir(self.argv)\n",
        "\n",
        "        \n",
        "        if argv.word_emb:\n",
        "            write('Loading Word Embeddings...')\n",
        "            word_list, word_emb = load_emb(argv.word_emb)\n",
        "            vocab_word = pproc.make_vocab_word(word_list)\n",
        "            write('\\t- # Vocabs: %d' % vocab_word.size())\n",
        "        else:\n",
        "            vocab_word = word_emb = None\n",
        "\n",
        "\n",
        "        \n",
        "        write('Loading Corpus...')\n",
        "        train_corpus = loader.load(path=argv.train_data,\n",
        "                                   data_size=argv.data_size,\n",
        "                                   is_test=False)\n",
        "        valid_corpus = loader.load(path=argv.dev_data,\n",
        "                                   data_size=argv.data_size,\n",
        "                                   is_test=False)\n",
        "        write('\\t- # Sents: Train:%d  Valid:%d' % (len(train_corpus), len(valid_corpus)))\n",
        "\n",
        "        \n",
        "        train_sents = pproc.make_sents(train_corpus)\n",
        "        valid_sents = pproc.make_sents(valid_corpus)\n",
        "\n",
        "        \n",
        "        write('Making Labels...')\n",
        "        vocab_label_train = pproc.make_and_save_vocab_label(sents=train_sents,\n",
        "                                                            vocab_label_init=None,\n",
        "                                                            save=argv.save,\n",
        "                                                            load=True)\n",
        "        vocab_label_valid = pproc.make_and_save_vocab_label(sents=valid_sents,\n",
        "                                                            vocab_label_init=vocab_label_train,\n",
        "                                                            save=False,\n",
        "                                                            load=False)\n",
        "        write('\\t- # Labels: %d' % vocab_label_train.size())\n",
        "\n",
        "        \n",
        "        train_sents = pproc.set_sent_config(sents=train_sents,\n",
        "                                            elmo_emb=train_elmo_emb,\n",
        "                                            vocab_word=vocab_word,\n",
        "                                            vocab_label=vocab_label_train)\n",
        "        valid_sents = pproc.set_sent_config(sents=valid_sents,\n",
        "                                            elmo_emb=valid_elmo_emb,\n",
        "                                            vocab_word=vocab_word,\n",
        "                                            vocab_label=vocab_label_valid)\n",
        "\n",
        "        \n",
        "        write('Making Samples...')\n",
        "        train_samples = pproc.make_samples(sents=train_sents,\n",
        "                                           is_valid_data=False)\n",
        "        valid_samples = pproc.make_samples(sents=valid_sents,\n",
        "                                           is_valid_data=True)\n",
        "        write('\\t- # Samples: Train:%d  Valid:%d' % (len(train_samples),\n",
        "                                                     len(valid_samples)))\n",
        "\n",
        "        \n",
        "        if train_elmo_emb is not None:\n",
        "            use_elmo = True\n",
        "        else:\n",
        "            use_elmo = False\n",
        "\n",
        "        if argv.n_experts > 0:\n",
        "            is_ensemble = True\n",
        "        else:\n",
        "            is_ensemble = False\n",
        "\n",
        "        if argv.method == 'span':\n",
        "            self.model_api.n_true_spans = count_true_spans(train_sents)\n",
        "\n",
        "        if is_ensemble:\n",
        "            self.model_api.set_ensemble_model(word_emb=word_emb,\n",
        "                                              use_elmo=use_elmo,\n",
        "                                              vocab_word=vocab_word,\n",
        "                                              vocab_label=vocab_label_train,\n",
        "                                              vocab_label_valid=vocab_label_valid)\n",
        "            self.model_api.load_experts_params(argv.load_param_dir)\n",
        "            self.model_api.set_init_ensemble_param()\n",
        "            self.model_api.set_ensemble_train_func()\n",
        "            if self.model_api.vocab_label_valid:\n",
        "                self.model_api.set_ensemble_pred_func()\n",
        "            init_epoch = 0\n",
        "        else:\n",
        "            self.model_api.set_model(word_emb=word_emb,\n",
        "                                     use_elmo=use_elmo,\n",
        "                                     vocab_word=vocab_word,\n",
        "                                     vocab_label=vocab_label_train,\n",
        "                                     vocab_label_valid=vocab_label_valid)\n",
        "            if argv.load_param_latest:\n",
        "                if argv.output_dir:\n",
        "                    dir_name = argv.output_dir\n",
        "                else:\n",
        "                    dir_name = 'output'\n",
        "                param_fns = get_file_names_in_dir(dir_path=dir_name,\n",
        "                                                  prefix='param')\n",
        "                opt_param_fns = get_file_names_in_dir(dir_path=dir_name,\n",
        "                                                      prefix='opt')\n",
        "                param_fn, latest_epoch = get_latest_param_fn(file_names=param_fns)\n",
        "                opt_param_fn, _ = get_latest_param_fn(file_names=opt_param_fns)\n",
        "                self.model_api.argv.load_param = param_fn\n",
        "                self.model_api.argv.load_opt_param = opt_param_fn\n",
        "                self.model_api.load_params(param_fn)\n",
        "                init_epoch = latest_epoch + 1\n",
        "            elif argv.load_param:\n",
        "                self.model_api.load_params(argv.load_param)\n",
        "                init_epoch = 0\n",
        "            else:\n",
        "                init_epoch = 0\n",
        "\n",
        "            self.model_api.set_train_func()\n",
        "            if self.model_api.vocab_label_valid:\n",
        "                self.model_api.set_pred_func()\n",
        "\n",
        "        \n",
        "        self._run_epochs(train_samples, valid_samples, init_epoch)\n",
        "\n",
        "    def _run_epochs(self, train_samples, valid_samples=None, init_epoch=0):\n",
        "        write('\\nTRAIN START')\n",
        "\n",
        "        argv = self.argv\n",
        "        pproc = self.preprocessor\n",
        "        vocab_label_valid = self.model_api.vocab_label_valid\n",
        "\n",
        "        if valid_samples:\n",
        "            valid_batches = pproc.make_batches(samples=valid_samples,\n",
        "                                               is_valid_data=True)\n",
        "            valid_batch_x, valid_batch_y = pproc.split_x_and_y(valid_batches)\n",
        "        else:\n",
        "            valid_batch_x = valid_batch_y = []\n",
        "\n",
        "        \n",
        "        if (argv.load_param or argv.load_param_dir) and valid_samples:\n",
        "            write('\\nEpoch: 0 (Using the Pre-trained Params)')\n",
        "            write('VALID')\n",
        "            valid_batch_y_pred = self.model_api.predict(valid_batch_x)\n",
        "            self.best_valid_f1 = self.evaluator.f_score(y_true=valid_batch_y,\n",
        "                                                        y_pred=valid_batch_y_pred,\n",
        "                                                        vocab_label=vocab_label_valid)\n",
        "\n",
        "        \n",
        "        for epoch in range(init_epoch, argv.epoch):\n",
        "            write('\\nEpoch: %d' % (epoch + 1))\n",
        "            write('TRAIN')\n",
        "\n",
        "            if argv.halve_lr and epoch > 49 and (epoch % 25) == 0:\n",
        "                lr = self.model_api.optimizer.lr.get_value(borrow=True)\n",
        "                self.model_api.optimizer.lr.set_value(lr * 0.5)\n",
        "                write('### HALVE LEARNING RATE: %f -> %f' % (lr, lr * 0.5))\n",
        "\n",
        "            \n",
        "            train_batches = pproc.make_batches(train_samples)\n",
        "            self.model_api.train(train_batches)\n",
        "\n",
        "            \n",
        "            if valid_samples:\n",
        "                write('VALID')\n",
        "                valid_batch_y_pred = self.model_api.predict(valid_batch_x)\n",
        "                valid_f1 = self.evaluator.f_score(y_true=valid_batch_y,\n",
        "                                                  y_pred=valid_batch_y_pred,\n",
        "                                                  vocab_label=vocab_label_valid)\n",
        "                if self.best_valid_f1 < valid_f1:\n",
        "                    self.best_valid_f1 = valid_f1\n",
        "                    self.best_epoch = epoch\n",
        "                    self.f1_history[self.best_epoch + 1] = [self.best_valid_f1]\n",
        "\n",
        "                    if argv.save:\n",
        "                        self.model_api.save_params(epoch=0)\n",
        "                        self.model_api.optimizer.save_params(epoch=0)\n",
        "\n",
        "            show_score_history(self.f1_history)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvsUhVdrSIhq"
      },
      "source": [
        "def parse_args(argument):\n",
        "    parser = argparse.ArgumentParser(description='SPAN SELECTION MODEL')\n",
        "\n",
        "    parser.add_argument('--mode', default='train', help='train/test')\n",
        "    \n",
        "    # Input Datasets \n",
        "    parser.add_argument('--train_data', help='path to train data')\n",
        "    parser.add_argument('--dev_data', help='path to dev data')\n",
        "    parser.add_argument('--test_data', help='path to test data')\n",
        "    parser.add_argument('--data_type', default='conll12', help='conll05/conll12')\n",
        "    parser.add_argument('--data_size', type=int, default=100000000, help='data size to be used')\n",
        "\n",
        "    \n",
        "    # Output Options \n",
        "    parser.add_argument('--save', action='store_true', default=False, help='parameters to be saved or not')\n",
        "    parser.add_argument('--output_dir', type=str, default='output', help='output directory name')\n",
        "    parser.add_argument('--output_fn', type=str, default=None, help='output file name')\n",
        "\n",
        "    # Search \n",
        "    parser.add_argument('--search', type=str, default='greedy', help='argmax/greedy')\n",
        "\n",
        "    \n",
        "    # NN Architecture \n",
        "    parser.add_argument('--emb_dim', type=int, default=50, help='dimension of embeddings')\n",
        "    parser.add_argument('--hidden_dim', type=int, default=32, help='dimension of hidden layer')\n",
        "    parser.add_argument('--n_layers', type=int, default=1, help='number of layers')\n",
        "    parser.add_argument('--n_experts', type=int, default=0, help='number of ensemble models')\n",
        "\n",
        "    # Training \n",
        "    parser.add_argument('--epoch', type=int, default=30, help='number of epochs to train')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='mini-batch size')\n",
        "    parser.add_argument('--word_emb', default=None, help='Initial embeddings to be loaded')\n",
        "\n",
        "    # Optimization \n",
        "    parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
        "    parser.add_argument('--halve_lr', action='store_true', default=False, help='halve learning rate')\n",
        "    parser.add_argument('--opt_type', default='adam', help='sgd/adam')\n",
        "    parser.add_argument('--grad_clip', action='store_true', default=False, help='gradient clipping')\n",
        "    parser.add_argument('--reg', type=float, default=0.0001, help='L2 Reg rate')\n",
        "    parser.add_argument('--drop_rate', type=float, default=0.0, help='Dropout Rate')\n",
        "\n",
        "    \n",
        "    # Loading Options \n",
        "    parser.add_argument('--load_param', default=None, help='path to params')\n",
        "    parser.add_argument('--load_param_dir', default=None, help='path to param dir')\n",
        "    parser.add_argument('--load_param_latest', action='store_true', default=False, help='load the latest params')\n",
        "    parser.add_argument('--load_opt_param', default=None, help='path to params')\n",
        "    parser.add_argument('--load_label', default=None, help='path to labels')\n",
        "\n",
        "    return parser.parse_args(argument.split())"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4nRU_7DWsjI",
        "outputId": "7e61dbea-6d24-4c12-c78c-1ae36493eda9"
      },
      "source": [
        "# Main part \n",
        "# training the model\n",
        "\n",
        "argv = parse_args(\"--method span --mode train --train_data train_data.txt --dev_data dev_data.txt --data_type conll012 --drop_rate 0.1 --reg 0.0001 --hidden_dim 300 --n_layers 4 --halve_lr --word_emb senna.emb.txt --save --output_dir output\")\n",
        "np.random.seed(argv.seed)\n",
        "\n",
        "loader = Conll12Loader(argv)\n",
        "\n",
        "Trainer(argv=argv,\n",
        "      loader=loader,\n",
        "      preprocessor=SpanPreprocessor(argv),\n",
        "      evaluator=SpanEvaluator(argv),\n",
        "      model_api=SpanModelAPI(argv)\n",
        "      ).train()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TRAINING START\n",
            "\n",
            "Loading Word Embeddings...\n",
            "\t- # Vocabs: 130000\n",
            "Loading Corpus...\n",
            "\t- # Sents: Train:749  Valid:819\n",
            "Making Labels...\n",
            "\t- # Labels: 28\n",
            "Making Samples...\n",
            "\t- # Samples: Train:1423  Valid:1942\n",
            "Setting a model...\n",
            "\t- EmbWord\n",
            "\t- EmbMark\n",
            "\t- BiRNNs-4:(100x300)\n",
            "\t- Dense(600x28,None)\n",
            "Model configuration\n",
            "\t- Input  Dim: 50\n",
            "\t- Hidden Dim: 300\n",
            "\t- Output Dim: -1\n",
            "\t- Parameters: 3326528\n",
            "Building a training function...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building a predicting function...\n",
            "\n",
            "TRAIN START\n",
            "\n",
            "Epoch: 1\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.885332 seconds\n",
            "\tAverage Negative Log Likelihood: 42.191060(3290.902646/78)\n",
            "\tF:  4.02%  P:  3.05% (  198/ 6489)  R:  5.89% (  198/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.639748 seconds\n",
            "\tF: 10.15%  P: 22.57% (  269/ 1192)  R:  6.55% (  269/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\n",
            "Epoch: 2\n",
            "TRAIN\n",
            "\n",
            "\tTime: 58.745191 seconds\n",
            "\tAverage Negative Log Likelihood: 13.068132(1019.314283/78)\n",
            "\tF: 22.87%  P: 35.41% (  568/ 1604)  R: 16.88% (  568/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.854769 seconds\n",
            "\tF: 17.46%  P: 44.60% (  446/ 1000)  R: 10.85% (  446/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\n",
            "Epoch: 3\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.140293 seconds\n",
            "\tAverage Negative Log Likelihood: 11.064164(863.004819/78)\n",
            "\tF: 30.55%  P: 43.58% (  791/ 1815)  R: 23.51% (  791/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.637875 seconds\n",
            "\tF: 28.68%  P: 41.87% (  896/ 2140)  R: 21.81% (  896/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\n",
            "Epoch: 4\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.505063 seconds\n",
            "\tAverage Negative Log Likelihood: 10.303553(803.677118/78)\n",
            "\tF: 34.19%  P: 46.12% (  914/ 1982)  R: 27.17% (  914/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.522048 seconds\n",
            "\tF: 29.25%  P: 43.81% (  902/ 2059)  R: 21.95% (  902/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\n",
            "Epoch: 5\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.957922 seconds\n",
            "\tAverage Negative Log Likelihood: 8.767781(683.886929/78)\n",
            "\tF: 39.58%  P: 52.72% ( 1066/ 2022)  R: 31.69% ( 1066/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.314678 seconds\n",
            "\tF: 25.38%  P: 37.43% (  789/ 2108)  R: 19.20% (  789/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\n",
            "Epoch: 6\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.322374 seconds\n",
            "\tAverage Negative Log Likelihood: 8.376506(653.367483/78)\n",
            "\tF: 41.77%  P: 52.97% ( 1160/ 2190)  R: 34.48% ( 1160/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.382172 seconds\n",
            "\tF: 31.79%  P: 43.37% ( 1031/ 2377)  R: 25.09% ( 1031/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\n",
            "Epoch: 7\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.422442 seconds\n",
            "\tAverage Negative Log Likelihood: 7.558717(589.579951/78)\n",
            "\tF: 46.68%  P: 57.34% ( 1324/ 2309)  R: 39.36% ( 1324/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.393916 seconds\n",
            "\tF: 34.67%  P: 47.04% ( 1128/ 2398)  R: 27.45% ( 1128/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\n",
            "Epoch: 8\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.422572 seconds\n",
            "\tAverage Negative Log Likelihood: 6.968188(543.518700/78)\n",
            "\tF: 49.15%  P: 58.77% ( 1421/ 2418)  R: 42.24% ( 1421/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.856854 seconds\n",
            "\tF: 35.08%  P: 43.80% ( 1202/ 2744)  R: 29.25% ( 1202/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\n",
            "Epoch: 9\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.762934 seconds\n",
            "\tAverage Negative Log Likelihood: 6.650793(518.761865/78)\n",
            "\tF: 50.86%  P: 60.88% ( 1469/ 2413)  R: 43.67% ( 1469/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.473826 seconds\n",
            "\tF: 30.45%  P: 46.82% (  927/ 1980)  R: 22.56% (  927/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\n",
            "Epoch: 10\n",
            "TRAIN\n",
            "\n",
            "\tTime: 58.591557 seconds\n",
            "\tAverage Negative Log Likelihood: 6.302582(491.601383/78)\n",
            "\tF: 55.58%  P: 66.04% ( 1614/ 2444)  R: 47.98% ( 1614/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.298722 seconds\n",
            "\tF: 36.11%  P: 44.06% ( 1257/ 2853)  R: 30.59% ( 1257/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\n",
            "Epoch: 11\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.714744 seconds\n",
            "\tAverage Negative Log Likelihood: 5.726179(446.641928/78)\n",
            "\tF: 58.25%  P: 67.12% ( 1731/ 2579)  R: 51.46% ( 1731/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.455231 seconds\n",
            "\tF: 33.73%  P: 48.28% ( 1065/ 2206)  R: 25.92% ( 1065/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\n",
            "Epoch: 12\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.572903 seconds\n",
            "\tAverage Negative Log Likelihood: 5.254555(409.855260/78)\n",
            "\tF: 61.27%  P: 69.84% ( 1836/ 2629)  R: 54.58% ( 1836/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.759714 seconds\n",
            "\tF: 36.92%  P: 52.25% ( 1173/ 2245)  R: 28.55% ( 1173/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\n",
            "Epoch: 13\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.114099 seconds\n",
            "\tAverage Negative Log Likelihood: 4.510028(351.782191/78)\n",
            "\tF: 63.98%  P: 71.25% ( 1953/ 2741)  R: 58.06% ( 1953/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.670062 seconds\n",
            "\tF: 35.43%  P: 50.18% ( 1125/ 2242)  R: 27.38% ( 1125/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\n",
            "Epoch: 14\n",
            "TRAIN\n",
            "\n",
            "\tTime: 56.981171 seconds\n",
            "\tAverage Negative Log Likelihood: 4.403500(343.472965/78)\n",
            "\tF: 65.96%  P: 74.26% ( 1996/ 2688)  R: 59.33% ( 1996/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.869231 seconds\n",
            "\tF: 37.86%  P: 48.37% ( 1278/ 2642)  R: 31.10% ( 1278/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\n",
            "Epoch: 15\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.495352 seconds\n",
            "\tAverage Negative Log Likelihood: 4.704708(366.967225/78)\n",
            "\tF: 66.30%  P: 72.82% ( 2047/ 2811)  R: 60.85% ( 2047/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.376290 seconds\n",
            "\tF: 37.31%  P: 51.66% ( 1200/ 2323)  R: 29.20% ( 1200/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\n",
            "Epoch: 16\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.674788 seconds\n",
            "\tAverage Negative Log Likelihood: 3.943879(307.622527/78)\n",
            "\tF: 71.40%  P: 78.48% ( 2203/ 2807)  R: 65.49% ( 2203/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.666969 seconds\n",
            "\tF: 36.34%  P: 47.95% ( 1202/ 2507)  R: 29.25% ( 1202/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\n",
            "Epoch: 17\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.372815 seconds\n",
            "\tAverage Negative Log Likelihood: 3.718108(290.012406/78)\n",
            "\tF: 72.43%  P: 78.65% ( 2258/ 2871)  R: 67.12% ( 2258/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.469313 seconds\n",
            "\tF: 39.68%  P: 46.44% ( 1423/ 3064)  R: 34.63% ( 1423/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\n",
            "Epoch: 18\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.860756 seconds\n",
            "\tAverage Negative Log Likelihood: 3.287138(256.396736/78)\n",
            "\tF: 74.52%  P: 80.03% ( 2345/ 2930)  R: 69.71% ( 2345/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.961389 seconds\n",
            "\tF: 39.71%  P: 49.89% ( 1355/ 2716)  R: 32.98% ( 1355/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\n",
            "Epoch: 19\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.538157 seconds\n",
            "\tAverage Negative Log Likelihood: 3.234806(252.314898/78)\n",
            "\tF: 75.80%  P: 80.47% ( 2410/ 2995)  R: 71.64% ( 2410/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.381311 seconds\n",
            "\tF: 37.38%  P: 48.96% ( 1242/ 2537)  R: 30.23% ( 1242/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\n",
            "Epoch: 20\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.616746 seconds\n",
            "\tAverage Negative Log Likelihood: 3.032641(236.546004/78)\n",
            "\tF: 77.74%  P: 83.04% ( 2458/ 2960)  R: 73.07% ( 2458/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.533748 seconds\n",
            "\tF: 39.25%  P: 50.93% ( 1312/ 2576)  R: 31.93% ( 1312/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\n",
            "Epoch: 21\n",
            "TRAIN\n",
            "\n",
            "\tTime: 57.712881 seconds\n",
            "\tAverage Negative Log Likelihood: 2.767951(215.900167/78)\n",
            "\tF: 79.79%  P: 83.90% ( 2559/ 3050)  R: 76.07% ( 2559/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.706805 seconds\n",
            "\tF: 40.03%  P: 50.29% ( 1366/ 2716)  R: 33.24% ( 1366/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\t- EPOCH-21  \tBEST VALID  40.03%\n",
            "\n",
            "Epoch: 22\n",
            "TRAIN\n",
            "\n",
            "\tTime: 59.644229 seconds\n",
            "\tAverage Negative Log Likelihood: 2.294057(178.936435/78)\n",
            "\tF: 82.48%  P: 86.60% ( 2649/ 3059)  R: 78.75% ( 2649/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.866093 seconds\n",
            "\tF: 41.72%  P: 50.14% ( 1468/ 2928)  R: 35.73% ( 1468/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\t- EPOCH-21  \tBEST VALID  40.03%\n",
            "\t- EPOCH-22  \tBEST VALID  41.72%\n",
            "\n",
            "Epoch: 23\n",
            "TRAIN\n",
            "\n",
            "\tTime: 59.440392 seconds\n",
            "\tAverage Negative Log Likelihood: 2.481261(193.538342/78)\n",
            "\tF: 83.04%  P: 86.85% ( 2676/ 3081)  R: 79.55% ( 2676/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 30.557985 seconds\n",
            "\tF: 40.79%  P: 49.77% ( 1420/ 2853)  R: 34.56% ( 1420/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\t- EPOCH-21  \tBEST VALID  40.03%\n",
            "\t- EPOCH-22  \tBEST VALID  41.72%\n",
            "\n",
            "Epoch: 24\n",
            "TRAIN\n",
            "\n",
            "\tTime: 60.964592 seconds\n",
            "\tAverage Negative Log Likelihood: 2.152221(167.873233/78)\n",
            "\tF: 85.96%  P: 89.69% ( 2776/ 3095)  R: 82.52% ( 2776/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 31.344279 seconds\n",
            "\tF: 39.57%  P: 49.13% ( 1361/ 2770)  R: 33.12% ( 1361/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\t- EPOCH-21  \tBEST VALID  40.03%\n",
            "\t- EPOCH-22  \tBEST VALID  41.72%\n",
            "\n",
            "Epoch: 25\n",
            "TRAIN\n",
            "\n",
            "\tTime: 63.142663 seconds\n",
            "\tAverage Negative Log Likelihood: 2.300623(179.448628/78)\n",
            "\tF: 85.03%  P: 88.24% ( 2760/ 3128)  R: 82.05% ( 2760/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 31.299635 seconds\n",
            "\tF: 40.21%  P: 49.21% ( 1397/ 2839)  R: 34.00% ( 1397/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\t- EPOCH-21  \tBEST VALID  40.03%\n",
            "\t- EPOCH-22  \tBEST VALID  41.72%\n",
            "\n",
            "Epoch: 26\n",
            "TRAIN\n",
            "\n",
            "\tTime: 63.644589 seconds\n",
            "\tAverage Negative Log Likelihood: 2.545593(198.556227/78)\n",
            "\tF: 84.51%  P: 88.02% ( 2734/ 3106)  R: 81.27% ( 2734/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 31.168814 seconds\n",
            "\tF: 39.11%  P: 46.75% ( 1381/ 2954)  R: 33.61% ( 1381/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\t- EPOCH-21  \tBEST VALID  40.03%\n",
            "\t- EPOCH-22  \tBEST VALID  41.72%\n",
            "\n",
            "Epoch: 27\n",
            "TRAIN\n",
            "\n",
            "\tTime: 64.400891 seconds\n",
            "\tAverage Negative Log Likelihood: 2.231337(174.044303/78)\n",
            "\tF: 85.35%  P: 88.66% ( 2768/ 3122)  R: 82.28% ( 2768/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 31.389061 seconds\n",
            "\tF: 41.50%  P: 50.07% ( 1456/ 2908)  R: 35.43% ( 1456/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\t- EPOCH-21  \tBEST VALID  40.03%\n",
            "\t- EPOCH-22  \tBEST VALID  41.72%\n",
            "\n",
            "Epoch: 28\n",
            "TRAIN\n",
            "\n",
            "\tTime: 63.815397 seconds\n",
            "\tAverage Negative Log Likelihood: 2.337886(182.355082/78)\n",
            "\tF: 84.80%  P: 87.86% ( 2757/ 3138)  R: 81.96% ( 2757/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 31.259069 seconds\n",
            "\tF: 41.40%  P: 49.46% ( 1463/ 2958)  R: 35.60% ( 1463/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\t- EPOCH-21  \tBEST VALID  40.03%\n",
            "\t- EPOCH-22  \tBEST VALID  41.72%\n",
            "\n",
            "Epoch: 29\n",
            "TRAIN\n",
            "\n",
            "\tTime: 63.693534 seconds\n",
            "\tAverage Negative Log Likelihood: 1.975266(154.070771/78)\n",
            "\tF: 87.44%  P: 90.68% ( 2840/ 3132)  R: 84.42% ( 2840/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 31.590093 seconds\n",
            "\tF: 40.70%  P: 47.41% ( 1465/ 3090)  R: 35.65% ( 1465/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\t- EPOCH-21  \tBEST VALID  40.03%\n",
            "\t- EPOCH-22  \tBEST VALID  41.72%\n",
            "\n",
            "Epoch: 30\n",
            "TRAIN\n",
            "\n",
            "\tTime: 63.874132 seconds\n",
            "\tAverage Negative Log Likelihood: 1.700738(132.657597/78)\n",
            "\tF: 90.72%  P: 93.20% ( 2973/ 3190)  R: 88.38% ( 2973/ 3364)\n",
            "VALID\n",
            "\n",
            "\tTime: 31.671879 seconds\n",
            "\tF: 41.11%  P: 49.55% ( 1443/ 2912)  R: 35.12% ( 1443/ 4109)\n",
            "F1 HISTORY\n",
            "\t- EPOCH-1  \tBEST VALID  10.15%\n",
            "\t- EPOCH-2  \tBEST VALID  17.46%\n",
            "\t- EPOCH-3  \tBEST VALID  28.68%\n",
            "\t- EPOCH-4  \tBEST VALID  29.25%\n",
            "\t- EPOCH-6  \tBEST VALID  31.79%\n",
            "\t- EPOCH-7  \tBEST VALID  34.67%\n",
            "\t- EPOCH-8  \tBEST VALID  35.08%\n",
            "\t- EPOCH-10  \tBEST VALID  36.11%\n",
            "\t- EPOCH-12  \tBEST VALID  36.92%\n",
            "\t- EPOCH-14  \tBEST VALID  37.86%\n",
            "\t- EPOCH-17  \tBEST VALID  39.68%\n",
            "\t- EPOCH-18  \tBEST VALID  39.71%\n",
            "\t- EPOCH-21  \tBEST VALID  40.03%\n",
            "\t- EPOCH-22  \tBEST VALID  41.72%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmDNR-PMqES_"
      },
      "source": [
        "def load_emb(path):\n",
        "    word_list = []\n",
        "    emb = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip().split()\n",
        "            word_list.append(line[0])\n",
        "            emb.append(line[1:])\n",
        "    emb = np.asarray(emb, dtype=theano.config.floatX)\n",
        "\n",
        "    if UNK not in word_list:\n",
        "        word_list = [UNK] + word_list\n",
        "        unk_vector = np.mean(emb, axis=0)\n",
        "        emb = np.vstack((unk_vector, emb))\n",
        "\n",
        "    return word_list, emb\n",
        "\n",
        "def write(s, stream=sys.stdout):\n",
        "    stream.write(s + '\\n')\n",
        "    stream.flush()\n",
        "\n",
        "def make_vocab_from_ids(key_value_format):\n",
        "    vocab = Vocab()\n",
        "    for key, value in key_value_format:\n",
        "        vocab.add_word(key)\n",
        "    return vocab"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NExx_BAGZyP9"
      },
      "source": [
        "class Tester(object):\n",
        "    def __init__(self,\n",
        "                 argv,\n",
        "                 loader,\n",
        "                 saver,\n",
        "                 preprocessor,\n",
        "                 evaluator,\n",
        "                 model_api):\n",
        "        self.argv = argv\n",
        "        self.loader = loader\n",
        "        self.saver = saver\n",
        "        self.preprocessor = preprocessor\n",
        "        self.evaluator = evaluator\n",
        "        self.model_api = model_api\n",
        "\n",
        "    def predict(self):\n",
        "        argv = self.argv\n",
        "        pproc = self.preprocessor\n",
        "        loader = self.loader\n",
        "\n",
        "        \n",
        "        # Load dataset \n",
        "        write('Loading Dataset...')\n",
        "        test_corpus = loader.load(path=argv.test_data,\n",
        "                                  data_size=argv.data_size,\n",
        "                                  is_test=True)\n",
        "        test_sents = pproc.make_sents(test_corpus)\n",
        "\n",
        "        \n",
        "        # Load init emb \n",
        "        if argv.word_emb:\n",
        "            write('Loading Embeddings...')\n",
        "            word_list, word_emb = load_emb(argv.word_emb)\n",
        "            vocab_word = pproc.make_vocab_word(word_list)\n",
        "            write('\\t- # Embedding Words: %d' % vocab_word.size())\n",
        "        else:\n",
        "            vocab_word = word_emb = None\n",
        "\n",
        "        if argv.test_elmo_emb:\n",
        "            write('Loading ELMo Embeddings...')\n",
        "            test_elmo_emb = loader.load_hdf5(argv.test_elmo_emb)\n",
        "        else:\n",
        "            test_elmo_emb = None\n",
        "\n",
        "        \n",
        "        # Make labels \n",
        "        label_key_value = loader.load_key_value_format(argv.load_label)\n",
        "        vocab_label = make_vocab_from_ids(label_key_value)\n",
        "        write('\\t- # Labels: %d' % vocab_label.size())\n",
        "\n",
        "        \n",
        "        # Set sent params \n",
        "        test_sents = pproc.set_sent_config(sents=test_sents,\n",
        "                                           elmo_emb=test_elmo_emb,\n",
        "                                           vocab_word=vocab_word,\n",
        "                                           vocab_label=None)\n",
        "        \n",
        "        # Make samples \n",
        "        write('Making Test Samples...')\n",
        "        test_batches = pproc.make_batch_per_sent(sents=test_sents)\n",
        "        write('\\t- # Test Samples: %d' % len(test_batches))\n",
        "\n",
        "        \n",
        "        # Model API \n",
        "        use_elmo = True if test_elmo_emb is not None else False\n",
        "\n",
        "        if argv.n_experts > 0:\n",
        "            self.model_api.set_ensemble_model(word_emb=word_emb,\n",
        "                                              use_elmo=use_elmo,\n",
        "                                              vocab_word=vocab_word,\n",
        "                                              vocab_label=vocab_label,\n",
        "                                              vocab_label_valid=None)\n",
        "            self.model_api.load_params(argv.load_param)\n",
        "            self.model_api.load_experts_params(argv.load_param_dir)\n",
        "            self.model_api.set_ensemble_pred_func()\n",
        "        else:\n",
        "            self.model_api.set_model(word_emb=word_emb,\n",
        "                                     use_elmo=use_elmo,\n",
        "                                     vocab_word=vocab_word,\n",
        "                                     vocab_label=vocab_label,\n",
        "                                     vocab_label_valid=None)\n",
        "            self.model_api.load_params(argv.load_param)\n",
        "            self.model_api.set_pred_func()\n",
        "\n",
        "        \n",
        "        # Testing \n",
        "        write('\\nPREDICTION START')\n",
        "        test_y_pred = self.model_api.predict(test_batches)\n",
        "        self.saver.save_props(corpus=test_sents,\n",
        "                              labels=test_y_pred,\n",
        "                              vocab_label=vocab_label)\n",
        "        self.saver.save_json_format(corpus=test_sents,\n",
        "                                    labels=test_y_pred,\n",
        "                                    vocab_label=vocab_label)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBP8IJT3qj8G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ1XbZP4qC74"
      },
      "source": [
        "class Saver(object):\n",
        "\n",
        "    def __init__(self, argv):\n",
        "        self.argv = argv\n",
        "\n",
        "    def save_props(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save_json_format(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "        \n",
        "class SpanSaver(Saver):\n",
        "    def save_props(self, corpus, labels, vocab_label):\n",
        "        \"\"\"\n",
        "        :param corpus: 1D: n_sents, 2D: n_words; elem=line\n",
        "        :param labels: 1D: n_sents, 2D: n_prds, 3D: n_spans, 4D: [r, i, j]\n",
        "        \"\"\"\n",
        "        assert len(corpus) == len(labels), '%d %d' % (len(corpus), len(labels))\n",
        "\n",
        "        fn = self.argv.output_dir\n",
        "        if self.argv.output_fn:\n",
        "            fn += '/results.%s.prop' % self.argv.output_fn\n",
        "        else:\n",
        "            fn += '/results.prop'\n",
        "        f = open(fn, 'w')\n",
        "\n",
        "        for sent, spans_sent in zip(corpus, labels):\n",
        "            columns = [[mark] for mark in sent.marks]\n",
        "            n_words = sent.n_words\n",
        "            assert len(sent.prd_indices) == len(spans_sent)\n",
        "            for prd_index, spans in zip(sent.prd_indices, spans_sent):\n",
        "                prop = self._span_to_prop(spans=spans,\n",
        "                                          prd_index=prd_index,\n",
        "                                          n_words=n_words,\n",
        "                                          vocab_label=vocab_label)\n",
        "                for i, p in enumerate(prop):\n",
        "                    columns[i].append(p)\n",
        "            for c in columns:\n",
        "                f.write(\"%s\\n\" % \"\\t\".join(c))\n",
        "            f.write(\"\\n\")\n",
        "        f.close()\n",
        "\n",
        "    def save_json_format(self, corpus, labels, vocab_label):\n",
        "        \"\"\"\n",
        "        :param corpus: 1D: n_sents, 2D: n_words; elem=line\n",
        "        :param labels: 1D: n_sents, 2D: n_prds, 3D: n_spans, 4D: [r, i, j]\n",
        "        :param vocab_label: Vocab()\n",
        "        \"\"\"\n",
        "        assert len(corpus) == len(labels), '%d %d' % (len(corpus), len(labels))\n",
        "\n",
        "        fn = self.argv.output_dir\n",
        "        if self.argv.output_fn:\n",
        "            fn += '/results.%s.json' % self.argv.output_fn\n",
        "        else:\n",
        "            fn += '/results.json'\n",
        "        f = open(fn, 'w')\n",
        "\n",
        "        corpus_dic = {}\n",
        "        for sent_index, (sent, spans_sent) in enumerate(zip(corpus, labels)):\n",
        "            assert len(sent.prd_indices) == len(spans_sent)\n",
        "\n",
        "            prop_dic = {}\n",
        "            for prd_index, spans in zip(sent.prd_indices, spans_sent):\n",
        "                arg_dic = {}\n",
        "                for (r, i, j) in spans:\n",
        "                    key = '(%s,%d,%d)' % (vocab_label.get_word(r), i, j)\n",
        "                    value = \" \".join(sent.strings[i: j + 1])\n",
        "                    arg_dic[key] = value\n",
        "\n",
        "                prd_dic = {'prd': sent.forms[prd_index],\n",
        "                           'arg': arg_dic}\n",
        "                prop_dic['prd-%d' % prd_index] = prd_dic\n",
        "\n",
        "            sent_dic = {'text': \" \".join(sent.strings),\n",
        "                        'mark': \" \".join(sent.marks),\n",
        "                        'prop': prop_dic}\n",
        "            corpus_dic['sent-%d' % sent_index] = sent_dic\n",
        "\n",
        "        json.dump(corpus_dic, f, indent=4)\n",
        "        f.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def _span_to_prop(spans, prd_index, n_words, vocab_label):\n",
        "        \"\"\"\n",
        "        :param spans: 1D: n_spans, 2D: [r, i, j]\n",
        "        :return: 1D: n_words; elem=str; e.g. (A0* & *)\n",
        "        \"\"\"\n",
        "        prop = ['*' for _ in range(n_words)]\n",
        "        prop[prd_index] = '(V*)'\n",
        "        for (label_id, pre_index, post_index) in spans:\n",
        "            label = vocab_label.get_word(label_id)\n",
        "            if pre_index == post_index:\n",
        "                prop[pre_index] = '(%s*)' % label\n",
        "            else:\n",
        "                prop[pre_index] = '(%s*' % label\n",
        "                prop[post_index] = '*)'\n",
        "        return prop"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STPFybkPqi4W",
        "outputId": "e95c1b75-a611-4e27-fb87-b7f5877e59b8"
      },
      "source": [
        "# testing the model\n",
        "argv = parse_args(\"--method span --mode test --test_data test_data_short.txt --data_type conll12 --drop_rate 0.1 --hidden_dim 300 --n_layers 4 --output_dir output --output_fn conll2005.test --word_emb senna.emb.txt --load_label output/label_ids.txt --load_param output/param.epoch-0.pkl.gz\")\n",
        "np.random.seed(argv.seed)\n",
        "\n",
        "Tester(argv=argv,\n",
        "                   loader=loader,\n",
        "                   saver=SpanSaver(argv),\n",
        "                   preprocessor=SpanPreprocessor(argv),\n",
        "                   evaluator=SpanEvaluator(argv),\n",
        "                   model_api=SpanModelAPI(argv)\n",
        "                   ).predict()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Dataset...\n",
            "Loading Embeddings...\n",
            "\t- # Embedding Words: 130000\n",
            "\t- # Labels: 28\n",
            "Making Test Samples...\n",
            "\t- # Test Samples: 430\n",
            "Setting a model...\n",
            "\t- EmbWord\n",
            "\t- EmbMark\n",
            "\t- BiRNNs-4:(100x300)\n",
            "\t- Dense(600x28,None)\n",
            "Model configuration\n",
            "\t- Input  Dim: 50\n",
            "\t- Hidden Dim: 300\n",
            "\t- Output Dim: -1\n",
            "\t- Parameters: 3326528\n",
            "Building a predicting function...\n",
            "\n",
            "PREDICTION START\n",
            "100 200 300 400 \n",
            "\tTime: 35.937697 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6RL-fYDFlqY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}